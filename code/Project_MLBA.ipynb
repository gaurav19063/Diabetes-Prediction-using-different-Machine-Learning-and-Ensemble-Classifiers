{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_MLBA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAnfnSYaH2k9"
      },
      "source": [
        "# Diabetes Prediction Using Ensembling of Different Machine Learning Classifiers \n",
        "##The python Library we used are :\n",
        "\n",
        "<ul>\n",
        "<li>Numpy</li>\n",
        "<li>Pandas</li>\n",
        "<li>scipy</li>\n",
        "<li>Seaborn</li>\n",
        "<li>Sklearn</li>\n",
        "<li>xgboost</li>\n",
        "</ul>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kj82opbJHlmR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "b88630bc-14df-4756-b217-fbfa2e3c9774"
      },
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier as RFC\n",
        "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score,matthews_corrcoef\n",
        "import numpy as np\n",
        "from sklearn.model_selection import cross_validate\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report,f1_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.neighbors  import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier, GradientBoostingClassifier,VotingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import xgboost as xgb\n",
        "from scipy import stats\n",
        "from scipy.stats import uniform, randint\n",
        "from sklearn.model_selection import KFold, StratifiedKFold, RepeatedStratifiedKFold\n",
        "from sklearn.metrics import roc_curve, auc, accuracy_score,roc_auc_score\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from scipy import interp\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.decomposition import FastICA\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers import Activation, Dense, Dropout, BatchNormalization, Input\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vldC7ibIRG4"
      },
      "source": [
        "# thsis function removes the outliers.\n",
        "def Outlier_Removal_IQR (data):\n",
        "  for col in data.columns:                               \n",
        "    C1 = data[col].quantile(0.25)\n",
        "    C3 = data[col].quantile(0.75)\n",
        "    IQR = C3-C1                                         \n",
        "    LB = (C1 - 1.5 * IQR)                                   #find lower boundary\n",
        "    UB = (C3 + 1.5 * IQR)                                   #find upper boundary\n",
        "    data = data[data[col]<UB]                #drop greater than upper limit\n",
        "    data = data[data[col]>LB]                #drop smaller than lower limit\n",
        "\n",
        "  return data\n",
        "  #this  function is for algorithm based feature selection \n",
        "def feature_Selection(data, selector, n_feature):\n",
        "    if selector=='SelectKbest':\n",
        "        X_train= data.iloc[:,:8]\n",
        "        Y_train=data.iloc[:,8:]\n",
        "        select=SelectKBest(chi2, k=n_feature) \n",
        "        select.fit_transform(X_train, Y_train)\n",
        "        cols = select.get_support(indices=True)\n",
        "        X_train_selected = X_train.iloc[:,cols]\n",
        "        # print(X_train_selected.columns)\n",
        "        return X_train_selected,Y_train\n",
        "    if selector  == 'None':\n",
        "        return data.iloc[:,:8].values, data.iloc[:,8:].values            \n",
        "    if selector=='PCA':                                                  \n",
        "        X_Data= data.iloc[:,:8].values\n",
        "        pca = PCA(n_components=n_feature)                             \n",
        "        X_Data = pca.fit_transform(X_Data)\n",
        "        return X_Data , data.iloc[:,8:].values\n",
        "    if selector  == 'None':\n",
        "        return data.iloc[:,:8].values, data.iloc[:,8:].values           \n",
        "    if selector =='corr':                                                   \n",
        "        if n_feature ==6:\n",
        "            data.drop(['BloodPressure','Age'],axis=1,inplace=True)\n",
        "            return data.iloc[:,:6].values, data.iloc[:,6:].values"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "JCc7WT2AJTzr",
        "outputId": "464b5999-b8a7-4592-8e0d-891a9d2f7882"
      },
      "source": [
        "data=pd.read_csv(\"/content/drive/MyDrive/MLBA Project/diabetes.csv\")\n",
        "data.head()\n",
        "# data.describe()\n",
        "# data.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Pregnancies  Glucose  BloodPressure  ...  DiabetesPedigreeFunction  Age  Outcome\n",
              "0            6      148             72  ...                     0.627   50        1\n",
              "1            1       85             66  ...                     0.351   31        0\n",
              "2            8      183             64  ...                     0.672   32        1\n",
              "3            1       89             66  ...                     0.167   21        0\n",
              "4            0      137             40  ...                     2.288   33        1\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdPwV1lBJ0sZ"
      },
      "source": [
        "<h2> Data Competency Assessment</h2>\n",
        "<ul>\n",
        "<li>Checking Null values.</li>\n",
        "<li>Finding correlation between the different features.</li>\n",
        "<li>Finding Best features for the best model performance.</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "e1P75vGQJrw6",
        "outputId": "55cd7030-b38b-43d5-d305-5357a2a74301"
      },
      "source": [
        "# =========================== Finding null value =======================================\n",
        "print(\"Is there any Null value? \",data.isnull().any().any())\n",
        "# =========================== plotting class distribution =======================================\n",
        "sns.countplot(data['Outcome'])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Is there any Null value?  False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fd1e85d7cf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TbybN3VJ-Bd"
      },
      "source": [
        "<h3> Observations from the above Countplot.</h3>\n",
        "<ul>\n",
        "<li> There is the problem of class imbalancy (using oversampling techniques to handel this problem). </li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzJd3giQJ4uO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "6502d7f6-c863-460e-8bfe-5bc7bb0f1e24"
      },
      "source": [
        "# =========================== Cheacking Corrilation Between the different Features =======================================\n",
        "# sns.heatmap(data,annot=True, fmt=\"g\", cmap='viridis')\n",
        "corr = data.corr()\n",
        "sns.heatmap(corr, xticklabels=corr.columns,yticklabels=corr.columns,annot=True)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fd1e7ca88d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23d_WClTKIZo"
      },
      "source": [
        "### Preprocessing Selection\n",
        "---\n",
        "Process Selector | Preprocessing | \n",
        "---|---|\n",
        "P|Filling Missing Value|\n",
        "Q|Outlier Removal|\n",
        "R|Standardization|\n",
        "S|Feature Selection|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bDeD7cZLVSo"
      },
      "source": [
        "\n",
        "---\n",
        "Process Selector | Preprocessing | \n",
        "---|---|\n",
        "P|Filling Missing Value|"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "9H6DejgJKB-c",
        "outputId": "271008b8-93a0-44ef-ae3f-e64d6d1ad19c"
      },
      "source": [
        "data.fillna(data.mean())\n",
        "print()\n",
        "  "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQzg-wHUKpzj"
      },
      "source": [
        "---\n",
        "Process Selector | Preprocessing | \n",
        "---|---|\n",
        "Q|Outlier Removal|"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Ae-YPXhuKjDj",
        "outputId": "6d2d71f8-5960-46e3-d516-b191b73bbbf1"
      },
      "source": [
        "print('Shape Before Outlier Removal: ' + str(data.shape))\n",
        "data = Outlier_Removal_IQR  (data)\n",
        "\n",
        "print('Shape After Outlier Removal: ' + str(data.shape))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape Before Outlier Removal: (768, 9)\n",
            "Shape After Outlier Removal: (636, 9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1gyS-v3Wj00"
      },
      "source": [
        "def replace_zero(data, col, target):\n",
        "    \n",
        "    mean_by_target = data.loc[data[col] != 0, [col, target]].groupby(target).mean()\n",
        "    data.loc[(data[col] == 0)&(data[target] == 0), col] = mean_by_target.iloc[0][0]\n",
        "    data.loc[(data[col] == 0)&(data[target] == 1), col] = mean_by_target.iloc[1][0]\n",
        "for x in ['BloodPressure'\t,'SkinThickness',\t'Insulin',\t'BMI',\t'DiabetesPedigreeFunction']:  \n",
        "    replace_zero(data, x, 'Outcome')   "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtUn4wMlNbXb"
      },
      "source": [
        "---\n",
        "Process Selector | Preprocessing | \n",
        "---|---|\n",
        "R|Standardization|"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LVxTxxYMow0"
      },
      "source": [
        "# # =========================== Scaling data  =======================================\n",
        "# scaler = StandardScaler()\n",
        "# # data_train=data.drop(['Outcome'],axis=1)\n",
        "# scaled_data=scaler.fit_transform(data.drop(['Outcome'],axis=1))\n",
        "# X= pd.DataFrame(scaled_data, index=data.index, columns=list(data.columns)[:-1])\n",
        "# Y=data['Outcome']\n",
        "# print(X)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2p6zNAMWPG2"
      },
      "source": [
        "---\n",
        "Process Selector | Preprocessing | \n",
        "---|---|\n",
        "S|Feature Selection|"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "0DSpBHzzNjan",
        "outputId": "9b9fb60c-78dc-4abe-c8d7-2b6b414bcd60"
      },
      "source": [
        "print('Shape Before feature selection: ' + str(data.shape))\n",
        "X_train,Y_train = feature_Selection(data, selector='corr', n_feature=6)    \n",
        "print('Shape After Feature Selection: ' + str(X_train.shape))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape Before feature selection: (636, 9)\n",
            "Shape After Feature Selection: (636, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Rq3akq_t8eD"
      },
      "source": [
        "# model creation using GridSearchCV\n",
        "def model_creation (classifier, X_Train, Y_Train, tuned_parameters, verbose):\n",
        "  clf = GridSearchCV(classifier, tuned_parameters,verbose=verbose, cv=5, scoring='roc_auc',n_jobs=-1)\n",
        "  clf.fit(X_Train, Y_Train)\n",
        "  return clf"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nJ_EigGVXEm"
      },
      "source": [
        "# This function is used to calculate the different metrics\n",
        "def metrics (y_true, y_pred, probas_):\n",
        "  marks=n_dots*'-'\n",
        "  print(marks)\n",
        "  fpr, tpr, thresholds = roc_curve(y_true, probas_[:, 1])\n",
        "  tprs.append(interp(mean_fpr, fpr, tpr))\n",
        "  tprs[-1][0] = 0.0\n",
        "  roc_auc = auc(fpr, tpr)\n",
        "  print(\"classification report for current fold:\\n\")\n",
        "  print(classification_report(y_true, y_pred))\n",
        "  print(\"Area Under ROC (AUC) for the current fold: {}\".format(roc_auc))\n",
        "  print('Confusion Matrix for current fold: ')\n",
        "  print(confusion_matrix(y_true, y_pred))\n",
        "  print(\"Accuracy for Current Fold: {}\".format(accuracy_score(y_true, y_pred)))\n",
        "  tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "  return  tn, fp, fn, tp, roc_auc, fpr, tpr\n",
        "# It make avarages of all the lists and print them in a formatted manner\n",
        "def average_scores(aucs,Accuracy,TP,TN,FP,FN): \n",
        "  print()\n",
        "  n_dotsav=(n_dots-len('Average'))//2\n",
        "  print('-'*n_dotsav+'Average'+'-'*n_dotsav)\n",
        "  print(\"AUC (Avg.) is  %0.3f\" %(np.mean(aucs)))\n",
        "  print(\"Accuracy (Avg.) is  %0.3f\" %(np.mean(Accuracy)))\n",
        "  cm = [[int(np.mean(TP)), int(np.mean(FP))],[int(np.mean(FN))]]\n",
        "  print ('Avg. CM is '+str(cm))\n",
        "  cm = [[int(np.sum(TP)), int(np.sum(FP))],[int(np.sum(FN)), int(np.sum(TN))]]\n",
        "  print ('Total for all folds CM is '+str(cm))\n",
        "  re_auc=str(round(np.mean(aucs), 3))+'+/-'+str(round(np.std(aucs),3))\n",
        "  all_clf_res.append(re_auc)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkxO9_i1UL_J"
      },
      "source": [
        "'''\n",
        "Defining different models and tunning their hyperparameters using GridsearchCV.\n",
        "It creates different models on best parameters and return the model to evaluate.\n",
        "\n",
        "'''\n",
        "def model_Ensemble(n_model, X_Train, Y_Train, X_Test, Y_Test, weight):\n",
        "        \n",
        "    parameters_rf = {'criterion': ['gini','entropy']}         \n",
        "\n",
        "    mrf = model_creation (classifier = RandomForestClassifier(random_state=random_initializer),X_Train = X_Train,  Y_Train = Y_Train,\n",
        "                      tuned_parameters = parameters_rf,\n",
        "                      verbose=0) \n",
        "                                                                          \n",
        "    n_neighbors = [1,7,9,11,13,15,21,23,25,27,29,31,33,35,41,43,45,47,49]\n",
        "    leaf_size = [5,10,15,20,25,30,35,40,45,50]\n",
        "    Distance = [1,2]\n",
        "                                                                    \n",
        "    parameters_knn = [ {'n_neighbors': n_neighbors, 'algorithm' : ['brute'],'p':Distance},{'n_neighbors': n_neighbors, 'algorithm' : ['ball_tree'],'leaf_size' : leaf_size,\n",
        "                        'p':Distance},{'n_neighbors': n_neighbors, 'algorithm' : ['kd_tree'],'leaf_size' : leaf_size,'p':Distance}]\n",
        "    parameters_dt = {'criterion': ['gini','entropy'],'splitter': ['best'],'min_samples_split':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0],'min_samples_leaf': [1,2,3,4,5] }\n",
        "    mdt =model_creation (classifier = DecisionTreeClassifier(random_state=random_initializer), X_Train = X_Train,   Y_Train = Y_Train, tuned_parameters = parameters_dt,verbose=0)  #create model with DecisionTree classifier with utility function \n",
        "\n",
        "    mknn = model_creation (classifier = KNeighborsClassifier(),X_Train = X_Train,Y_Train = Y_Train,tuned_parameters = parameters_knn,verbose=0)                         #create knn with  function     \n",
        "    \n",
        "    parameters_ab = { 'algorithm': ['SAMME','SAMME.R'],  'learning_rate':[0.1,0.5,1.0], 'n_estimators': [10,50,100,200]} # define parameters of adaboost\n",
        "\n",
        "    mab =model_creation (classifier = AdaBoostClassifier(random_state=random_initializer), X_Train = X_Train, Y_Train = Y_Train,tuned_parameters =parameters_ab,verbose=0)\n",
        "    \n",
        "    parameters_nb = [{'var_smoothing': [1e-01,1e-02,1e-03,1e-04,1e-05,1e-06,1e-07,1e-08,1e-09, 1e-10, 1e-11, 1e-12]}]         # define parameters of Naive Bais\n",
        "\n",
        "    mnb = model_creation (classifier = GaussianNB(), X_Train = X_Train, Y_Train = Y_Train,tuned_parameters = parameters_nb,verbose=0)\n",
        "    parameters_xb = {      'min_child_weight': [1, 5, 10],'gamma': [0.5, 1, 1.5, 2, 5],'subsample': [0.5, 1.0],'colsample_bytree': [0.6, 0.8, 1.0],'max_depth': [3, 4, 5] }\n",
        "\n",
        "        \n",
        "    mxb =model_creation (classifier = xgb.XGBClassifier(objective = \"binary:logistic\", eval_metric = 'error',random_state=random_initializer), X_Train = X_Train, Y_Train = Y_Train,tuned_parameters = parameters_xb,verbose=0)\n",
        "    \n",
        "    \n",
        "\n",
        "    models=[('knn', mknn), ('xb',mxb),('dt',mdt),('rf',mrf),('nb',mnb),('ab',mab)]\n",
        "    if weight == 'accuracy':\n",
        "        if n_model == 2:                                               \n",
        "            model_index=[5,1]                                            \n",
        "            model = VotingClassifier( [models[i] for i in model_index], voting='soft', weights=[accuracy_score(Y_Test, mab.predict(X_Test)),accuracy_score(Y_Test, mxb.predict(X_Test))])\n",
        "\n",
        "            model.fit(X_Train,Y_Train)\n",
        "            return model \n",
        "        if n_model == 3:                                               \n",
        "            model_index=[0,1,2]                                              \n",
        "            model = VotingClassifier( [models[i] for i in model_index] , voting='soft', weights=[accuracy_score(Y_Test, mknn.predict(X_Test)),accuracy_score(Y_Test, mb.predict(X_Test)), accuracy_score(Y_Test, mdt.predict(X_Test))])   \n",
        "\n",
        "            model.fit(X_Train,Y_Train)\n",
        "            return model\n",
        "        if n_model == 4:    \n",
        "            model_index=[5,1,2,3]                                              \n",
        "            model = VotingClassifier( [models[i] for i in model_index] , voting='soft', weights=[accuracy_score(Y_Test, mab.predict(X_Test)),accuracy_score(Y_Test, mxb.predict(X_Test)),accuracy_score(Y_Test, mdt.predict(X_Test)),accuracy_score(Y_Test, mrf.predict(X_Test))])                                           #Using 6 best model we create ensembled model with soft voting    \n",
        "\n",
        "            model.fit(X_Train,Y_Train)\n",
        "            return model\n",
        "        if n_model == 5:     \n",
        "            model_index=[0,1,2,3,4]                                              \n",
        "            model = VotingClassifier( [models[i] for i in model_index] , voting='soft',weights=[accuracy_score(Y_Test, mknn.predict(X_Test)), accuracy_score(Y_Test, mxb.predict(X_Test)),accuracy_score(Y_Test, mdt.predict(X_Test)),accuracy_score(Y_Test, mrf.predict(X_Test)), accuracy_score(Y_Test, mnb.predict(X_Test))])                                           #Using 6 best model we create ensembled model with soft voting    \n",
        "\n",
        "            model.fit(X_Train,Y_Train)\n",
        "            return model\n",
        "\n",
        "\n",
        "    if weight == 'None':                                              \n",
        "        if n_model == 2:\n",
        "            model_index=[5,1]                                              \n",
        "            model = VotingClassifier( [models[i] for i in model_index], voting='soft')\n",
        "            model.fit(X_Train,Y_Train)\n",
        "            return model \n",
        "            \n",
        "        if n_model == 3:                                               \n",
        "\n",
        "            model_index=[0,1,2]                                              \n",
        "            model = VotingClassifier( [models[i] for i in model_index] , voting='soft')\n",
        "            model.fit(X_Train,Y_Train)\n",
        "            return model\n",
        "        if n_model == 4:                                               \n",
        "\n",
        "            model_index=[5,1,2,3]                                              \n",
        "            model = VotingClassifier( [models[i] for i in model_index] , voting='soft') \n",
        "            model.fit(X_Train,Y_Train)\n",
        "            return model\n",
        "        if n_model == 5:                                               \n",
        "            \n",
        "            \n",
        "            model_index=[0,1,2,3,4]                                              \n",
        "            model = VotingClassifier( [models[i] for i in model_index] , voting='soft') \n",
        "            model.fit(X_Train,Y_Train)\n",
        "            return model\n",
        "        if n_model == 6:                                               \n",
        "            \n",
        "            \n",
        "            model_index=[0,1,2,3,4,5]                                              \n",
        "            model = VotingClassifier( [models[i] for i in model_index] , voting='soft') \n",
        "            model.fit(X_Train,Y_Train)\n",
        "            return model\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rkEXhClYZfB"
      },
      "source": [
        "def Make_model_seq(activation,dropout_rate,init,learn_rate):\n",
        "\n",
        "    neuron1,neuron2,neuron3,neuron4=64,16,64,64\n",
        "\n",
        "    \n",
        "    model = Sequential()\n",
        "    np.random.seed(6)\n",
        "    model.add(Dense(64, input_dim =6, kernel_initializer= init, activation= activation))\n",
        "    model.add(Dense(16, input_dim = neuron1, kernel_initializer= init, activation= activation))\n",
        "    model.add(Dense(64, input_dim = neuron2, kernel_initializer= init, activation= activation))\n",
        "    model.add(Dense(64, input_dim = neuron3, kernel_initializer= init, activation= activation))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "    \n",
        "    optimizer = Adam(lr = learn_rate)               #optimizer of Neural network \n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy']) #compile model \n",
        "    #################################################################################################\n",
        "    return model     \n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "b2fAAFeZqOL9",
        "outputId": "262ff0cf-4b1e-4b79-e564-b81b8ef4a1ce"
      },
      "source": [
        "\n",
        "'''\n",
        "Here all the ensamble model prepares for different 5 fold and all the accuracy metrics stores in there corresponding lists.\n",
        "Here is the description of the different lists.\n",
        "FP: False positives\n",
        "TN: True Negatives\n",
        "TP: True Positives\n",
        "TN: True negatives\n",
        "sn: sencetivity\n",
        "sp: specificity\n",
        "'''\n",
        "all_clf_res=[]                    \n",
        "random_initializer=20            \n",
        "n_dots=70\n",
        "# X_Data=X_train\n",
        "# Y_Lavel=Y_train\n",
        "# retu\n",
        "\n",
        "for i in range(2,8):\n",
        "    Accuracy = [] \n",
        "    tprs = []                                                                  \n",
        "    aucs_ens = []                                                              \n",
        "    sn = []                                                                    \n",
        "    sp = []                                                                     \n",
        "    pr = []                                                                      \n",
        "    FOR = []                                                                   \n",
        "    DOR = []                                                              \n",
        "    FP = []                                                                    \n",
        "    TN = []                                                                \n",
        "    FN = []                                                               \n",
        "    TP = []                                                                 \n",
        "                                                                       \n",
        "    iterator=0\n",
        "    mean_fpr = np.linspace(0, 1, 100)\n",
        "    plus_print=n_dots*'#'\n",
        "    print(plus_print)\n",
        "    print('model running with ensembling model.(No of models used: ---  '+str(i )+' )')\n",
        "    print(plus_print)\n",
        "    kf = StratifiedKFold(n_splits=5,\n",
        "                     shuffle=False,\n",
        "                     random_state=random_initializer)\n",
        "    \n",
        "    for train_index, test_index in kf.split(X_train,Y_train):                    # split data in train,test\n",
        "        X_Train, X_Test = X_train[train_index], X_train[test_index]               # the train data and label\n",
        "        Y_Train, Y_Test = Y_train[train_index], Y_train[test_index]             # the  test data and label\n",
        "\n",
        "\n",
        "        if i<7:\n",
        "          # creating different ensembling models for each fold and finding there performance on the different metrics\n",
        "          clf = model_Ensemble( i, X_Train, Y_Train, X_Test, Y_Test, 'None')    \n",
        "          tn, fp, fn, tp, roc_auc, fpr, tpr = metrics (y_true = Y_Test,           #evaluation parameters of ensembelled model \n",
        "                                            y_pred = clf.predict(X_Test),\n",
        "                                            probas_ = clf.predict_proba(X_Test))       \n",
        "        else:\n",
        "          activation=\"relu\"\n",
        "          batch_size=8\n",
        "          epochs=200\n",
        "          learn_rate=.001\n",
        "          dropout_rate=0.6\n",
        "          init=\"normal\"\n",
        "          neuron1,neuron2,neuron3,neuron4=64,16,64,64\n",
        "          Y_Train_cat = to_categorical(Y_Train,2)                                #convert train output to catagorical\n",
        "          Y_Test_cat= to_categorical(Y_Test,2)                                  #convert test output to catagorical\n",
        "\n",
        "          model =Make_model_seq(activation, dropout_rate, init,learn_rate)    \n",
        "          np.random.seed(6)\n",
        "          model.fit(x=X_Train,  y=Y_Train_cat,batch_size=batch_size,epochs=epochs,shuffle=False,verbose=1)\n",
        "            \n",
        "          probas_ = model.predict(X_Test)                                           \n",
        "          print(type(probas_))\n",
        "          print(probas_.shape)\n",
        "          y_pred = np.argmax(model.predict(X_Test), axis=1)   \n",
        "\n",
        "          tn, fp, fn, tp, roc_auc, fpr, tpr = metrics (y_true = Y_Test,           \n",
        "                                            y_pred = y_pred,\n",
        "                                            probas_ = probas_)\n",
        "\n",
        "  \n",
        "\n",
        "        tprs.append(interp(mean_fpr, fpr, tpr))\n",
        "        tprs[-1][0] = 0.0\n",
        "        aucs_ens.append(roc_auc)\n",
        "        TN.append(tn)\n",
        "        FP.append(fp)\n",
        "        FN.append(fn)\n",
        "        TP.append(tp)\n",
        "        FOR.append(fn/(tn+fn))\n",
        "        DOR.append((tp*tn)/(fp*fn))\n",
        "        sn.append(tp/(tp+fn))\n",
        "        sp.append(tn/(fp+tn))\n",
        "        pr.append(tp/(tp+fp))\n",
        "        Accuracy.append(accuracy_score(Y_Test, clf.predict(X_Test)))\n",
        "    \n",
        "    # Overall performance of ensemble models\n",
        "    average_scores(aucs_ens,Accuracy,TP,TN,FP,FN)                         \n",
        "    print(\"Precision (Avg. ) is  %0.3f \" %(np.mean(pr))) \n",
        "    print(\"Sensitivity (Avg. ) is  %0.3f \" %(np.mean(sn)))\n",
        "    print(\"Specificity (Avg. ) is  %0.3f \" %(np.mean(sp)))\n",
        "    print(\"FOR (Avg.) is  %0.3f \" %(np.mean(FOR)))\n",
        "    print(\"DOR (Avg. ) is  %0.3f \" %(np.mean(DOR)))\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "######################################################################\n",
            "model running with ensembling model.(No of models used: ---  2 )\n",
            "######################################################################\n",
            "----------------------------------------------------------------------\n",
            "classification report for current fold:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.97      0.94        88\n",
            "           1       0.91      0.80      0.85        40\n",
            "\n",
            "    accuracy                           0.91       128\n",
            "   macro avg       0.91      0.88      0.90       128\n",
            "weighted avg       0.91      0.91      0.91       128\n",
            "\n",
            "Area Under ROC (AUC) for the current fold: 0.9491477272727273\n",
            "Confusion Matrix for current fold: \n",
            "[[85  3]\n",
            " [ 8 32]]\n",
            "Accuracy for Current Fold: 0.9140625\n",
            "----------------------------------------------------------------------\n",
            "classification report for current fold:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.93      0.90        87\n",
            "           1       0.82      0.70      0.76        40\n",
            "\n",
            "    accuracy                           0.86       127\n",
            "   macro avg       0.85      0.82      0.83       127\n",
            "weighted avg       0.86      0.86      0.85       127\n",
            "\n",
            "Area Under ROC (AUC) for the current fold: 0.9304597701149425\n",
            "Confusion Matrix for current fold: \n",
            "[[81  6]\n",
            " [12 28]]\n",
            "Accuracy for Current Fold: 0.8582677165354331\n",
            "----------------------------------------------------------------------\n",
            "classification report for current fold:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.93      0.92        87\n",
            "           1       0.84      0.78      0.81        40\n",
            "\n",
            "    accuracy                           0.88       127\n",
            "   macro avg       0.87      0.85      0.86       127\n",
            "weighted avg       0.88      0.88      0.88       127\n",
            "\n",
            "Area Under ROC (AUC) for the current fold: 0.9520114942528735\n",
            "Confusion Matrix for current fold: \n",
            "[[81  6]\n",
            " [ 9 31]]\n",
            "Accuracy for Current Fold: 0.8818897637795275\n",
            "----------------------------------------------------------------------\n",
            "classification report for current fold:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.93      0.93        88\n",
            "           1       0.84      0.82      0.83        39\n",
            "\n",
            "    accuracy                           0.90       127\n",
            "   macro avg       0.88      0.88      0.88       127\n",
            "weighted avg       0.90      0.90      0.90       127\n",
            "\n",
            "Area Under ROC (AUC) for the current fold: 0.9586247086247086\n",
            "Confusion Matrix for current fold: \n",
            "[[82  6]\n",
            " [ 7 32]]\n",
            "Accuracy for Current Fold: 0.8976377952755905\n",
            "----------------------------------------------------------------------\n",
            "classification report for current fold:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.95      0.94        88\n",
            "           1       0.89      0.82      0.85        39\n",
            "\n",
            "    accuracy                           0.91       127\n",
            "   macro avg       0.91      0.89      0.90       127\n",
            "weighted avg       0.91      0.91      0.91       127\n",
            "\n",
            "Area Under ROC (AUC) for the current fold: 0.9562937062937062\n",
            "Confusion Matrix for current fold: \n",
            "[[84  4]\n",
            " [ 7 32]]\n",
            "Accuracy for Current Fold: 0.9133858267716536\n",
            "\n",
            "-------------------------------Average-------------------------------\n",
            "AUC (Avg.) is  0.949\n",
            "Accuracy (Avg.) is  0.893\n",
            "Avg. CM is [[31, 5], [8]]\n",
            "Total for all folds CM is [[155, 25], [43, 413]]\n",
            "Precision (Avg. ) is  0.861 \n",
            "Sensitivity (Avg. ) is  0.783 \n",
            "Specificity (Avg. ) is  0.943 \n",
            "FOR (Avg.) is  0.094 \n",
            "DOR (Avg. ) is  69.962 \n",
            "######################################################################\n",
            "model running with ensembling model.(No of models used: ---  3 )\n",
            "######################################################################\n",
            "----------------------------------------------------------------------\n",
            "classification report for current fold:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.98      0.95        88\n",
            "           1       0.94      0.80      0.86        40\n",
            "\n",
            "    accuracy                           0.92       128\n",
            "   macro avg       0.93      0.89      0.90       128\n",
            "weighted avg       0.92      0.92      0.92       128\n",
            "\n",
            "Area Under ROC (AUC) for the current fold: 0.944034090909091\n",
            "Confusion Matrix for current fold: \n",
            "[[86  2]\n",
            " [ 8 32]]\n",
            "Accuracy for Current Fold: 0.921875\n",
            "----------------------------------------------------------------------\n",
            "classification report for current fold:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.93      0.90        87\n",
            "           1       0.82      0.70      0.76        40\n",
            "\n",
            "    accuracy                           0.86       127\n",
            "   macro avg       0.85      0.82      0.83       127\n",
            "weighted avg       0.86      0.86      0.85       127\n",
            "\n",
            "Area Under ROC (AUC) for the current fold: 0.92816091954023\n",
            "Confusion Matrix for current fold: \n",
            "[[81  6]\n",
            " [12 28]]\n",
            "Accuracy for Current Fold: 0.8582677165354331\n",
            "----------------------------------------------------------------------\n",
            "classification report for current fold:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.92      0.91        87\n",
            "           1       0.82      0.78      0.79        40\n",
            "\n",
            "    accuracy                           0.87       127\n",
            "   macro avg       0.86      0.85      0.85       127\n",
            "weighted avg       0.87      0.87      0.87       127\n",
            "\n",
            "Area Under ROC (AUC) for the current fold: 0.9477011494252873\n",
            "Confusion Matrix for current fold: \n",
            "[[80  7]\n",
            " [ 9 31]]\n",
            "Accuracy for Current Fold: 0.8740157480314961\n",
            "----------------------------------------------------------------------\n",
            "classification report for current fold:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.93      0.92        88\n",
            "           1       0.84      0.79      0.82        39\n",
            "\n",
            "    accuracy                           0.89       127\n",
            "   macro avg       0.87      0.86      0.87       127\n",
            "weighted avg       0.89      0.89      0.89       127\n",
            "\n",
            "Area Under ROC (AUC) for the current fold: 0.9536713286713286\n",
            "Confusion Matrix for current fold: \n",
            "[[82  6]\n",
            " [ 8 31]]\n",
            "Accuracy for Current Fold: 0.889763779527559\n",
            "----------------------------------------------------------------------\n",
            "classification report for current fold:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.91      0.91        88\n",
            "           1       0.80      0.82      0.81        39\n",
            "\n",
            "    accuracy                           0.88       127\n",
            "   macro avg       0.86      0.86      0.86       127\n",
            "weighted avg       0.88      0.88      0.88       127\n",
            "\n",
            "Area Under ROC (AUC) for the current fold: 0.9440559440559441\n",
            "Confusion Matrix for current fold: \n",
            "[[80  8]\n",
            " [ 7 32]]\n",
            "Accuracy for Current Fold: 0.8818897637795275\n",
            "\n",
            "-------------------------------Average-------------------------------\n",
            "AUC (Avg.) is  0.944\n",
            "Accuracy (Avg.) is  0.885\n",
            "Avg. CM is [[30, 5], [8]]\n",
            "Total for all folds CM is [[154, 29], [44, 409]]\n",
            "Precision (Avg. ) is  0.844 \n",
            "Sensitivity (Avg. ) is  0.778 \n",
            "Specificity (Avg. ) is  0.934 \n",
            "FOR (Avg.) is  0.097 \n",
            "DOR (Avg. ) is  68.308 \n",
            "######################################################################\n",
            "model running with ensembling model.(No of models used: ---  4 )\n",
            "######################################################################\n",
            "----------------------------------------------------------------------\n",
            "classification report for current fold:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.95      0.93        88\n",
            "           1       0.89      0.80      0.84        40\n",
            "\n",
            "    accuracy                           0.91       128\n",
            "   macro avg       0.90      0.88      0.89       128\n",
            "weighted avg       0.91      0.91      0.90       128\n",
            "\n",
            "Area Under ROC (AUC) for the current fold: 0.9417613636363636\n",
            "Confusion Matrix for current fold: \n",
            "[[84  4]\n",
            " [ 8 32]]\n",
            "Accuracy for Current Fold: 0.90625\n",
            "----------------------------------------------------------------------\n",
            "classification report for current fold:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.94      0.90        87\n",
            "           1       0.84      0.68      0.75        40\n",
            "\n",
            "    accuracy                           0.86       127\n",
            "   macro avg       0.85      0.81      0.83       127\n",
            "weighted avg       0.86      0.86      0.85       127\n",
            "\n",
            "Area Under ROC (AUC) for the current fold: 0.9295977011494253\n",
            "Confusion Matrix for current fold: \n",
            "[[82  5]\n",
            " [13 27]]\n",
            "Accuracy for Current Fold: 0.8582677165354331\n",
            "----------------------------------------------------------------------\n",
            "classification report for current fold:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.93      0.91        87\n",
            "           1       0.83      0.75      0.79        40\n",
            "\n",
            "    accuracy                           0.87       127\n",
            "   macro avg       0.86      0.84      0.85       127\n",
            "weighted avg       0.87      0.87      0.87       127\n",
            "\n",
            "Area Under ROC (AUC) for the current fold: 0.9494252873563218\n",
            "Confusion Matrix for current fold: \n",
            "[[81  6]\n",
            " [10 30]]\n",
            "Accuracy for Current Fold: 0.8740157480314961\n",
            "----------------------------------------------------------------------\n",
            "classification report for current fold:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.95      0.93        88\n",
            "           1       0.89      0.79      0.84        39\n",
            "\n",
            "    accuracy                           0.91       127\n",
            "   macro avg       0.90      0.87      0.89       127\n",
            "weighted avg       0.90      0.91      0.90       127\n",
            "\n",
            "Area Under ROC (AUC) for the current fold: 0.9516317016317016\n",
            "Confusion Matrix for current fold: \n",
            "[[84  4]\n",
            " [ 8 31]]\n",
            "Accuracy for Current Fold: 0.905511811023622\n",
            "----------------------------------------------------------------------\n",
            "classification report for current fold:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.97      0.94        88\n",
            "           1       0.91      0.79      0.85        39\n",
            "\n",
            "    accuracy                           0.91       127\n",
            "   macro avg       0.91      0.88      0.89       127\n",
            "weighted avg       0.91      0.91      0.91       127\n",
            "\n",
            "Area Under ROC (AUC) for the current fold: 0.9507575757575758\n",
            "Confusion Matrix for current fold: \n",
            "[[85  3]\n",
            " [ 8 31]]\n",
            "Accuracy for Current Fold: 0.9133858267716536\n",
            "\n",
            "-------------------------------Average-------------------------------\n",
            "AUC (Avg.) is  0.945\n",
            "Accuracy (Avg.) is  0.891\n",
            "Avg. CM is [[30, 4], [9]]\n",
            "Total for all folds CM is [[151, 22], [47, 416]]\n",
            "Precision (Avg. ) is  0.873 \n",
            "Sensitivity (Avg. ) is  0.763 \n",
            "Specificity (Avg. ) is  0.950 \n",
            "FOR (Avg.) is  0.101 \n",
            "DOR (Avg. ) is  69.946 \n",
            "######################################################################\n",
            "model running with ensembling model.(No of models used: ---  5 )\n",
            "######################################################################\n",
            "----------------------------------------------------------------------\n",
            "classification report for current fold:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.94      0.93        88\n",
            "           1       0.86      0.80      0.83        40\n",
            "\n",
            "    accuracy                           0.90       128\n",
            "   macro avg       0.89      0.87      0.88       128\n",
            "weighted avg       0.90      0.90      0.90       128\n",
            "\n",
            "Area Under ROC (AUC) for the current fold: 0.9372159090909091\n",
            "Confusion Matrix for current fold: \n",
            "[[83  5]\n",
            " [ 8 32]]\n",
            "Accuracy for Current Fold: 0.8984375\n",
            "----------------------------------------------------------------------\n",
            "classification report for current fold:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.94      0.90        87\n",
            "           1       0.84      0.68      0.75        40\n",
            "\n",
            "    accuracy                           0.86       127\n",
            "   macro avg       0.85      0.81      0.83       127\n",
            "weighted avg       0.86      0.86      0.85       127\n",
            "\n",
            "Area Under ROC (AUC) for the current fold: 0.928735632183908\n",
            "Confusion Matrix for current fold: \n",
            "[[82  5]\n",
            " [13 27]]\n",
            "Accuracy for Current Fold: 0.8582677165354331\n",
            "----------------------------------------------------------------------\n",
            "classification report for current fold:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.91      0.90        87\n",
            "           1       0.79      0.75      0.77        40\n",
            "\n",
            "    accuracy                           0.86       127\n",
            "   macro avg       0.84      0.83      0.83       127\n",
            "weighted avg       0.86      0.86      0.86       127\n",
            "\n",
            "Area Under ROC (AUC) for the current fold: 0.9442528735632185\n",
            "Confusion Matrix for current fold: \n",
            "[[79  8]\n",
            " [10 30]]\n",
            "Accuracy for Current Fold: 0.8582677165354331\n",
            "----------------------------------------------------------------------\n",
            "classification report for current fold:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.93      0.92        88\n",
            "           1       0.84      0.79      0.82        39\n",
            "\n",
            "    accuracy                           0.89       127\n",
            "   macro avg       0.87      0.86      0.87       127\n",
            "weighted avg       0.89      0.89      0.89       127\n",
            "\n",
            "Area Under ROC (AUC) for the current fold: 0.9504662004662005\n",
            "Confusion Matrix for current fold: \n",
            "[[82  6]\n",
            " [ 8 31]]\n",
            "Accuracy for Current Fold: 0.889763779527559\n",
            "----------------------------------------------------------------------\n",
            "classification report for current fold:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.93      0.93        88\n",
            "           1       0.84      0.82      0.83        39\n",
            "\n",
            "    accuracy                           0.90       127\n",
            "   macro avg       0.88      0.88      0.88       127\n",
            "weighted avg       0.90      0.90      0.90       127\n",
            "\n",
            "Area Under ROC (AUC) for the current fold: 0.9458041958041958\n",
            "Confusion Matrix for current fold: \n",
            "[[82  6]\n",
            " [ 7 32]]\n",
            "Accuracy for Current Fold: 0.8976377952755905\n",
            "\n",
            "-------------------------------Average-------------------------------\n",
            "AUC (Avg.) is  0.941\n",
            "Accuracy (Avg.) is  0.880\n",
            "Avg. CM is [[30, 6], [9]]\n",
            "Total for all folds CM is [[152, 30], [46, 408]]\n",
            "Precision (Avg. ) is  0.836 \n",
            "Sensitivity (Avg. ) is  0.768 \n",
            "Specificity (Avg. ) is  0.931 \n",
            "FOR (Avg.) is  0.101 \n",
            "DOR (Avg. ) is  49.104 \n",
            "######################################################################\n",
            "model running with ensembling model.(No of models used: ---  6 )\n",
            "######################################################################\n",
            "----------------------------------------------------------------------\n",
            "classification report for current fold:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.94      0.93        88\n",
            "           1       0.86      0.80      0.83        40\n",
            "\n",
            "    accuracy                           0.90       128\n",
            "   macro avg       0.89      0.87      0.88       128\n",
            "weighted avg       0.90      0.90      0.90       128\n",
            "\n",
            "Area Under ROC (AUC) for the current fold: 0.9380681818181819\n",
            "Confusion Matrix for current fold: \n",
            "[[83  5]\n",
            " [ 8 32]]\n",
            "Accuracy for Current Fold: 0.8984375\n",
            "----------------------------------------------------------------------\n",
            "classification report for current fold:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.94      0.90        87\n",
            "           1       0.84      0.68      0.75        40\n",
            "\n",
            "    accuracy                           0.86       127\n",
            "   macro avg       0.85      0.81      0.83       127\n",
            "weighted avg       0.86      0.86      0.85       127\n",
            "\n",
            "Area Under ROC (AUC) for the current fold: 0.9293103448275862\n",
            "Confusion Matrix for current fold: \n",
            "[[82  5]\n",
            " [13 27]]\n",
            "Accuracy for Current Fold: 0.8582677165354331\n",
            "----------------------------------------------------------------------\n",
            "classification report for current fold:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.90      0.89        87\n",
            "           1       0.77      0.75      0.76        40\n",
            "\n",
            "    accuracy                           0.85       127\n",
            "   macro avg       0.83      0.82      0.83       127\n",
            "weighted avg       0.85      0.85      0.85       127\n",
            "\n",
            "Area Under ROC (AUC) for the current fold: 0.9445402298850576\n",
            "Confusion Matrix for current fold: \n",
            "[[78  9]\n",
            " [10 30]]\n",
            "Accuracy for Current Fold: 0.8503937007874016\n",
            "----------------------------------------------------------------------\n",
            "classification report for current fold:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.93      0.92        88\n",
            "           1       0.84      0.79      0.82        39\n",
            "\n",
            "    accuracy                           0.89       127\n",
            "   macro avg       0.87      0.86      0.87       127\n",
            "weighted avg       0.89      0.89      0.89       127\n",
            "\n",
            "Area Under ROC (AUC) for the current fold: 0.9516317016317016\n",
            "Confusion Matrix for current fold: \n",
            "[[82  6]\n",
            " [ 8 31]]\n",
            "Accuracy for Current Fold: 0.889763779527559\n",
            "----------------------------------------------------------------------\n",
            "classification report for current fold:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.93      0.93        88\n",
            "           1       0.84      0.82      0.83        39\n",
            "\n",
            "    accuracy                           0.90       127\n",
            "   macro avg       0.88      0.88      0.88       127\n",
            "weighted avg       0.90      0.90      0.90       127\n",
            "\n",
            "Area Under ROC (AUC) for the current fold: 0.9463869463869463\n",
            "Confusion Matrix for current fold: \n",
            "[[82  6]\n",
            " [ 7 32]]\n",
            "Accuracy for Current Fold: 0.8976377952755905\n",
            "\n",
            "-------------------------------Average-------------------------------\n",
            "AUC (Avg.) is  0.942\n",
            "Accuracy (Avg.) is  0.879\n",
            "Avg. CM is [[30, 6], [9]]\n",
            "Total for all folds CM is [[152, 31], [46, 407]]\n",
            "Precision (Avg. ) is  0.832 \n",
            "Sensitivity (Avg. ) is  0.768 \n",
            "Specificity (Avg. ) is  0.929 \n",
            "FOR (Avg.) is  0.101 \n",
            "DOR (Avg. ) is  48.379 \n",
            "######################################################################\n",
            "model running with ensembling model.(No of models used: ---  7 )\n",
            "######################################################################\n",
            "Epoch 1/200\n",
            "64/64 [==============================] - 1s 1ms/step - loss: 0.6654 - accuracy: 0.6545\n",
            "Epoch 2/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6188 - accuracy: 0.6853\n",
            "Epoch 3/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.5882 - accuracy: 0.7338\n",
            "Epoch 4/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.5741 - accuracy: 0.6886\n",
            "Epoch 5/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5509 - accuracy: 0.7811\n",
            "Epoch 6/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.5340 - accuracy: 0.7443\n",
            "Epoch 7/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.5332 - accuracy: 0.7725\n",
            "Epoch 8/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.5153 - accuracy: 0.7976\n",
            "Epoch 9/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5158 - accuracy: 0.7810\n",
            "Epoch 10/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4996 - accuracy: 0.7791\n",
            "Epoch 11/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4637 - accuracy: 0.8292\n",
            "Epoch 12/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4539 - accuracy: 0.8170\n",
            "Epoch 13/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4370 - accuracy: 0.8238\n",
            "Epoch 14/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4286 - accuracy: 0.8215\n",
            "Epoch 15/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4273 - accuracy: 0.8396\n",
            "Epoch 16/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4172 - accuracy: 0.8582\n",
            "Epoch 17/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3942 - accuracy: 0.8413\n",
            "Epoch 18/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4116 - accuracy: 0.8401\n",
            "Epoch 19/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3803 - accuracy: 0.8512\n",
            "Epoch 20/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4043 - accuracy: 0.8499\n",
            "Epoch 21/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3668 - accuracy: 0.8688\n",
            "Epoch 22/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3933 - accuracy: 0.8592\n",
            "Epoch 23/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3676 - accuracy: 0.8547\n",
            "Epoch 24/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3558 - accuracy: 0.8709\n",
            "Epoch 25/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3956 - accuracy: 0.8348\n",
            "Epoch 26/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3831 - accuracy: 0.8676\n",
            "Epoch 27/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3459 - accuracy: 0.8715\n",
            "Epoch 28/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3649 - accuracy: 0.8577\n",
            "Epoch 29/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3294 - accuracy: 0.8760\n",
            "Epoch 30/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3410 - accuracy: 0.8751\n",
            "Epoch 31/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3676 - accuracy: 0.8449\n",
            "Epoch 32/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3629 - accuracy: 0.8627\n",
            "Epoch 33/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3380 - accuracy: 0.8756\n",
            "Epoch 34/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3534 - accuracy: 0.8737\n",
            "Epoch 35/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3703 - accuracy: 0.8576\n",
            "Epoch 36/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3402 - accuracy: 0.8500\n",
            "Epoch 37/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3580 - accuracy: 0.8334\n",
            "Epoch 38/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3545 - accuracy: 0.8513\n",
            "Epoch 39/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3472 - accuracy: 0.8515\n",
            "Epoch 40/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3706 - accuracy: 0.8551\n",
            "Epoch 41/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3340 - accuracy: 0.8590\n",
            "Epoch 42/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3350 - accuracy: 0.8733\n",
            "Epoch 43/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3630 - accuracy: 0.8549\n",
            "Epoch 44/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3536 - accuracy: 0.8458\n",
            "Epoch 45/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3441 - accuracy: 0.8648\n",
            "Epoch 46/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3422 - accuracy: 0.8642\n",
            "Epoch 47/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3571 - accuracy: 0.8531\n",
            "Epoch 48/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3355 - accuracy: 0.8670\n",
            "Epoch 49/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3818 - accuracy: 0.8299\n",
            "Epoch 50/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3714 - accuracy: 0.8166\n",
            "Epoch 51/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3211 - accuracy: 0.8532\n",
            "Epoch 52/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3270 - accuracy: 0.8559\n",
            "Epoch 53/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3183 - accuracy: 0.8677\n",
            "Epoch 54/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3252 - accuracy: 0.8670\n",
            "Epoch 55/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3387 - accuracy: 0.8548\n",
            "Epoch 56/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3491 - accuracy: 0.8461\n",
            "Epoch 57/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3278 - accuracy: 0.8460\n",
            "Epoch 58/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3359 - accuracy: 0.8628\n",
            "Epoch 59/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3601 - accuracy: 0.8433\n",
            "Epoch 60/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3212 - accuracy: 0.8590\n",
            "Epoch 61/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3238 - accuracy: 0.8547\n",
            "Epoch 62/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3280 - accuracy: 0.8521\n",
            "Epoch 63/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3162 - accuracy: 0.8504\n",
            "Epoch 64/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3276 - accuracy: 0.8605\n",
            "Epoch 65/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3279 - accuracy: 0.8512\n",
            "Epoch 66/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3269 - accuracy: 0.8676\n",
            "Epoch 67/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3293 - accuracy: 0.8618\n",
            "Epoch 68/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3320 - accuracy: 0.8434\n",
            "Epoch 69/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3278 - accuracy: 0.8607\n",
            "Epoch 70/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2871 - accuracy: 0.8575\n",
            "Epoch 71/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3272 - accuracy: 0.8611\n",
            "Epoch 72/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3197 - accuracy: 0.8528\n",
            "Epoch 73/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3128 - accuracy: 0.8495\n",
            "Epoch 74/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2862 - accuracy: 0.8806\n",
            "Epoch 75/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3306 - accuracy: 0.8618\n",
            "Epoch 76/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3371 - accuracy: 0.8700\n",
            "Epoch 77/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3260 - accuracy: 0.8769\n",
            "Epoch 78/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2892 - accuracy: 0.8817\n",
            "Epoch 79/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3037 - accuracy: 0.8857\n",
            "Epoch 80/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2980 - accuracy: 0.8701\n",
            "Epoch 81/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3050 - accuracy: 0.8722\n",
            "Epoch 82/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2919 - accuracy: 0.8843\n",
            "Epoch 83/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3217 - accuracy: 0.8594\n",
            "Epoch 84/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3140 - accuracy: 0.8847\n",
            "Epoch 85/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2784 - accuracy: 0.8856\n",
            "Epoch 86/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2899 - accuracy: 0.8721\n",
            "Epoch 87/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2987 - accuracy: 0.8814\n",
            "Epoch 88/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3306 - accuracy: 0.8866\n",
            "Epoch 89/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2955 - accuracy: 0.8715\n",
            "Epoch 90/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3124 - accuracy: 0.8789\n",
            "Epoch 91/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2956 - accuracy: 0.8669\n",
            "Epoch 92/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3121 - accuracy: 0.8807\n",
            "Epoch 93/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2805 - accuracy: 0.8905\n",
            "Epoch 94/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3147 - accuracy: 0.8598\n",
            "Epoch 95/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2895 - accuracy: 0.8726\n",
            "Epoch 96/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2909 - accuracy: 0.8653\n",
            "Epoch 97/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2957 - accuracy: 0.8753\n",
            "Epoch 98/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3122 - accuracy: 0.8705\n",
            "Epoch 99/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2910 - accuracy: 0.8672\n",
            "Epoch 100/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2833 - accuracy: 0.8633\n",
            "Epoch 101/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2697 - accuracy: 0.8758\n",
            "Epoch 102/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2740 - accuracy: 0.8856\n",
            "Epoch 103/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2907 - accuracy: 0.8602\n",
            "Epoch 104/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3163 - accuracy: 0.8769\n",
            "Epoch 105/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2955 - accuracy: 0.8513\n",
            "Epoch 106/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2844 - accuracy: 0.8818\n",
            "Epoch 107/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2931 - accuracy: 0.8616\n",
            "Epoch 108/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3037 - accuracy: 0.8555\n",
            "Epoch 109/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3033 - accuracy: 0.8599\n",
            "Epoch 110/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2744 - accuracy: 0.8869\n",
            "Epoch 111/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2747 - accuracy: 0.8713\n",
            "Epoch 112/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2971 - accuracy: 0.8841\n",
            "Epoch 113/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2885 - accuracy: 0.8672\n",
            "Epoch 114/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2862 - accuracy: 0.8948\n",
            "Epoch 115/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2973 - accuracy: 0.8679\n",
            "Epoch 116/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3022 - accuracy: 0.8801\n",
            "Epoch 117/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2699 - accuracy: 0.8846\n",
            "Epoch 118/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3026 - accuracy: 0.8809\n",
            "Epoch 119/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2761 - accuracy: 0.8661\n",
            "Epoch 120/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3098 - accuracy: 0.8771\n",
            "Epoch 121/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2893 - accuracy: 0.8658\n",
            "Epoch 122/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2942 - accuracy: 0.8766\n",
            "Epoch 123/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3091 - accuracy: 0.8682\n",
            "Epoch 124/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3464 - accuracy: 0.8760\n",
            "Epoch 125/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2885 - accuracy: 0.8768\n",
            "Epoch 126/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2647 - accuracy: 0.8847\n",
            "Epoch 127/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3528 - accuracy: 0.8513\n",
            "Epoch 128/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2869 - accuracy: 0.8640\n",
            "Epoch 129/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2788 - accuracy: 0.8996\n",
            "Epoch 130/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2704 - accuracy: 0.8683\n",
            "Epoch 131/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2938 - accuracy: 0.8733\n",
            "Epoch 132/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3204 - accuracy: 0.8709\n",
            "Epoch 133/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2758 - accuracy: 0.8959\n",
            "Epoch 134/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3186 - accuracy: 0.8752\n",
            "Epoch 135/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2811 - accuracy: 0.9041\n",
            "Epoch 136/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2771 - accuracy: 0.8886\n",
            "Epoch 137/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2893 - accuracy: 0.8708\n",
            "Epoch 138/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2606 - accuracy: 0.8908\n",
            "Epoch 139/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2693 - accuracy: 0.8807\n",
            "Epoch 140/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3036 - accuracy: 0.8720\n",
            "Epoch 141/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3079 - accuracy: 0.8535\n",
            "Epoch 142/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2963 - accuracy: 0.8794\n",
            "Epoch 143/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3009 - accuracy: 0.8860\n",
            "Epoch 144/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2640 - accuracy: 0.8843\n",
            "Epoch 145/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2625 - accuracy: 0.8690\n",
            "Epoch 146/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2638 - accuracy: 0.8821\n",
            "Epoch 147/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3013 - accuracy: 0.8664\n",
            "Epoch 148/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2914 - accuracy: 0.8746\n",
            "Epoch 149/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2997 - accuracy: 0.8627\n",
            "Epoch 150/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2853 - accuracy: 0.8597\n",
            "Epoch 151/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2755 - accuracy: 0.8856\n",
            "Epoch 152/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2947 - accuracy: 0.8871\n",
            "Epoch 153/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2635 - accuracy: 0.9031\n",
            "Epoch 154/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2919 - accuracy: 0.8778\n",
            "Epoch 155/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2792 - accuracy: 0.8620\n",
            "Epoch 156/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2761 - accuracy: 0.8692\n",
            "Epoch 157/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3047 - accuracy: 0.8643\n",
            "Epoch 158/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3236 - accuracy: 0.8810\n",
            "Epoch 159/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2741 - accuracy: 0.8533\n",
            "Epoch 160/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2807 - accuracy: 0.8695\n",
            "Epoch 161/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2976 - accuracy: 0.8788\n",
            "Epoch 162/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2579 - accuracy: 0.8886\n",
            "Epoch 163/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2821 - accuracy: 0.8716\n",
            "Epoch 164/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3082 - accuracy: 0.8886\n",
            "Epoch 165/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2812 - accuracy: 0.8835\n",
            "Epoch 166/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2712 - accuracy: 0.8882\n",
            "Epoch 167/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2725 - accuracy: 0.8707\n",
            "Epoch 168/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2684 - accuracy: 0.8897\n",
            "Epoch 169/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2637 - accuracy: 0.8770\n",
            "Epoch 170/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3387 - accuracy: 0.8716\n",
            "Epoch 171/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2654 - accuracy: 0.8909\n",
            "Epoch 172/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2678 - accuracy: 0.8895\n",
            "Epoch 173/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2746 - accuracy: 0.8835\n",
            "Epoch 174/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2821 - accuracy: 0.8882\n",
            "Epoch 175/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2781 - accuracy: 0.8870\n",
            "Epoch 176/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2758 - accuracy: 0.8817\n",
            "Epoch 177/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3048 - accuracy: 0.8664\n",
            "Epoch 178/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2894 - accuracy: 0.8929\n",
            "Epoch 179/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2492 - accuracy: 0.8815\n",
            "Epoch 180/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3287 - accuracy: 0.8907\n",
            "Epoch 181/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2782 - accuracy: 0.8814\n",
            "Epoch 182/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2577 - accuracy: 0.8881\n",
            "Epoch 183/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2626 - accuracy: 0.8865\n",
            "Epoch 184/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2601 - accuracy: 0.8924\n",
            "Epoch 185/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2418 - accuracy: 0.8845\n",
            "Epoch 186/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2588 - accuracy: 0.8872\n",
            "Epoch 187/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2690 - accuracy: 0.8922\n",
            "Epoch 188/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2750 - accuracy: 0.8797\n",
            "Epoch 189/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2827 - accuracy: 0.8865\n",
            "Epoch 190/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2746 - accuracy: 0.8789\n",
            "Epoch 191/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2714 - accuracy: 0.8724\n",
            "Epoch 192/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2633 - accuracy: 0.8933\n",
            "Epoch 193/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3084 - accuracy: 0.8820\n",
            "Epoch 194/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2706 - accuracy: 0.8858\n",
            "Epoch 195/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2548 - accuracy: 0.8944\n",
            "Epoch 196/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2709 - accuracy: 0.8757\n",
            "Epoch 197/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3224 - accuracy: 0.8693\n",
            "Epoch 198/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2495 - accuracy: 0.8922\n",
            "Epoch 199/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2538 - accuracy: 0.8950\n",
            "Epoch 200/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2631 - accuracy: 0.8821\n",
            "<class 'numpy.ndarray'>\n",
            "(128, 2)\n",
            "----------------------------------------------------------------------\n",
            "classification report for current fold:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.91      0.89        88\n",
            "           1       0.78      0.72      0.75        40\n",
            "\n",
            "    accuracy                           0.85       128\n",
            "   macro avg       0.83      0.82      0.82       128\n",
            "weighted avg       0.85      0.85      0.85       128\n",
            "\n",
            "Area Under ROC (AUC) for the current fold: 0.8863636363636362\n",
            "Confusion Matrix for current fold: \n",
            "[[80  8]\n",
            " [11 29]]\n",
            "Accuracy for Current Fold: 0.8515625\n",
            "Epoch 1/200\n",
            "64/64 [==============================] - 1s 2ms/step - loss: 0.6880 - accuracy: 0.5697\n",
            "Epoch 2/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6607 - accuracy: 0.6372\n",
            "Epoch 3/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6334 - accuracy: 0.6515\n",
            "Epoch 4/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5979 - accuracy: 0.6948\n",
            "Epoch 5/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5862 - accuracy: 0.7157\n",
            "Epoch 6/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5699 - accuracy: 0.7288\n",
            "Epoch 7/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5566 - accuracy: 0.7410\n",
            "Epoch 8/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.5624 - accuracy: 0.7348\n",
            "Epoch 9/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5477 - accuracy: 0.7484\n",
            "Epoch 10/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5204 - accuracy: 0.7643\n",
            "Epoch 11/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4970 - accuracy: 0.8097\n",
            "Epoch 12/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4926 - accuracy: 0.7727\n",
            "Epoch 13/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4839 - accuracy: 0.7869\n",
            "Epoch 14/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4524 - accuracy: 0.8115\n",
            "Epoch 15/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4796 - accuracy: 0.7610\n",
            "Epoch 16/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4435 - accuracy: 0.8120\n",
            "Epoch 17/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4691 - accuracy: 0.8144\n",
            "Epoch 18/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4408 - accuracy: 0.8364\n",
            "Epoch 19/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4491 - accuracy: 0.8235\n",
            "Epoch 20/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4251 - accuracy: 0.8292\n",
            "Epoch 21/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4304 - accuracy: 0.8669\n",
            "Epoch 22/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4274 - accuracy: 0.8280\n",
            "Epoch 23/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4146 - accuracy: 0.8306\n",
            "Epoch 24/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4200 - accuracy: 0.8135\n",
            "Epoch 25/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3888 - accuracy: 0.8304\n",
            "Epoch 26/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3765 - accuracy: 0.8453\n",
            "Epoch 27/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3997 - accuracy: 0.8348\n",
            "Epoch 28/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4231 - accuracy: 0.8515\n",
            "Epoch 29/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4045 - accuracy: 0.8340\n",
            "Epoch 30/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4041 - accuracy: 0.8338\n",
            "Epoch 31/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3749 - accuracy: 0.8436\n",
            "Epoch 32/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3811 - accuracy: 0.8569\n",
            "Epoch 33/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3812 - accuracy: 0.8488\n",
            "Epoch 34/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3934 - accuracy: 0.8390\n",
            "Epoch 35/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3868 - accuracy: 0.8279\n",
            "Epoch 36/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3613 - accuracy: 0.8471\n",
            "Epoch 37/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3813 - accuracy: 0.8444\n",
            "Epoch 38/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3520 - accuracy: 0.8497\n",
            "Epoch 39/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3571 - accuracy: 0.8466\n",
            "Epoch 40/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3485 - accuracy: 0.8570\n",
            "Epoch 41/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3949 - accuracy: 0.8523\n",
            "Epoch 42/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3895 - accuracy: 0.8505\n",
            "Epoch 43/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3615 - accuracy: 0.8549\n",
            "Epoch 44/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3453 - accuracy: 0.8676\n",
            "Epoch 45/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3597 - accuracy: 0.8519\n",
            "Epoch 46/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3692 - accuracy: 0.8162\n",
            "Epoch 47/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3708 - accuracy: 0.8290\n",
            "Epoch 48/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3703 - accuracy: 0.8474\n",
            "Epoch 49/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3286 - accuracy: 0.8670\n",
            "Epoch 50/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3658 - accuracy: 0.8479\n",
            "Epoch 51/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3418 - accuracy: 0.8624\n",
            "Epoch 52/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3577 - accuracy: 0.8546\n",
            "Epoch 53/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3360 - accuracy: 0.8385\n",
            "Epoch 54/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3976 - accuracy: 0.8563\n",
            "Epoch 55/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3509 - accuracy: 0.8590\n",
            "Epoch 56/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3468 - accuracy: 0.8519\n",
            "Epoch 57/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3507 - accuracy: 0.8234\n",
            "Epoch 58/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3431 - accuracy: 0.8439\n",
            "Epoch 59/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3348 - accuracy: 0.8488\n",
            "Epoch 60/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3486 - accuracy: 0.8438\n",
            "Epoch 61/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3815 - accuracy: 0.8432\n",
            "Epoch 62/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3731 - accuracy: 0.8518\n",
            "Epoch 63/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3320 - accuracy: 0.8544\n",
            "Epoch 64/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3592 - accuracy: 0.8502\n",
            "Epoch 65/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3531 - accuracy: 0.8661\n",
            "Epoch 66/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3980 - accuracy: 0.8261\n",
            "Epoch 67/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3498 - accuracy: 0.8491\n",
            "Epoch 68/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3566 - accuracy: 0.8568\n",
            "Epoch 69/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3408 - accuracy: 0.8463\n",
            "Epoch 70/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3489 - accuracy: 0.8471\n",
            "Epoch 71/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3605 - accuracy: 0.8499\n",
            "Epoch 72/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3425 - accuracy: 0.8558\n",
            "Epoch 73/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3463 - accuracy: 0.8695\n",
            "Epoch 74/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3572 - accuracy: 0.8348\n",
            "Epoch 75/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3418 - accuracy: 0.8423\n",
            "Epoch 76/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3267 - accuracy: 0.8576\n",
            "Epoch 77/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3446 - accuracy: 0.8635\n",
            "Epoch 78/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3356 - accuracy: 0.8748\n",
            "Epoch 79/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3189 - accuracy: 0.8574\n",
            "Epoch 80/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3226 - accuracy: 0.8741\n",
            "Epoch 81/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3279 - accuracy: 0.8590\n",
            "Epoch 82/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3138 - accuracy: 0.8645\n",
            "Epoch 83/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3319 - accuracy: 0.8635\n",
            "Epoch 84/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2963 - accuracy: 0.8770\n",
            "Epoch 85/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3522 - accuracy: 0.8504\n",
            "Epoch 86/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3552 - accuracy: 0.8580\n",
            "Epoch 87/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3301 - accuracy: 0.8583\n",
            "Epoch 88/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3101 - accuracy: 0.8602\n",
            "Epoch 89/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3265 - accuracy: 0.8628\n",
            "Epoch 90/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3310 - accuracy: 0.8643\n",
            "Epoch 91/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4137 - accuracy: 0.8437\n",
            "Epoch 92/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3592 - accuracy: 0.8651\n",
            "Epoch 93/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3355 - accuracy: 0.8561\n",
            "Epoch 94/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3345 - accuracy: 0.8608\n",
            "Epoch 95/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3393 - accuracy: 0.8623\n",
            "Epoch 96/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3196 - accuracy: 0.8536\n",
            "Epoch 97/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3341 - accuracy: 0.8729\n",
            "Epoch 98/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3400 - accuracy: 0.8582\n",
            "Epoch 99/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3217 - accuracy: 0.8534\n",
            "Epoch 100/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3598 - accuracy: 0.8630\n",
            "Epoch 101/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3239 - accuracy: 0.8518\n",
            "Epoch 102/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3179 - accuracy: 0.8628\n",
            "Epoch 103/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3111 - accuracy: 0.8519\n",
            "Epoch 104/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3306 - accuracy: 0.8596\n",
            "Epoch 105/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3253 - accuracy: 0.8626\n",
            "Epoch 106/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3410 - accuracy: 0.8609\n",
            "Epoch 107/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3021 - accuracy: 0.8705\n",
            "Epoch 108/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3563 - accuracy: 0.8618\n",
            "Epoch 109/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3566 - accuracy: 0.8412\n",
            "Epoch 110/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3184 - accuracy: 0.8652\n",
            "Epoch 111/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3294 - accuracy: 0.8412\n",
            "Epoch 112/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3563 - accuracy: 0.8473\n",
            "Epoch 113/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3264 - accuracy: 0.8488\n",
            "Epoch 114/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3327 - accuracy: 0.8602\n",
            "Epoch 115/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3197 - accuracy: 0.8710\n",
            "Epoch 116/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3037 - accuracy: 0.8891\n",
            "Epoch 117/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3264 - accuracy: 0.8581\n",
            "Epoch 118/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2993 - accuracy: 0.8630\n",
            "Epoch 119/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3306 - accuracy: 0.8707\n",
            "Epoch 120/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3050 - accuracy: 0.8718\n",
            "Epoch 121/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3146 - accuracy: 0.8655\n",
            "Epoch 122/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3135 - accuracy: 0.8673\n",
            "Epoch 123/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3462 - accuracy: 0.8605\n",
            "Epoch 124/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3215 - accuracy: 0.8594\n",
            "Epoch 125/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3074 - accuracy: 0.8603\n",
            "Epoch 126/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3095 - accuracy: 0.8627\n",
            "Epoch 127/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3148 - accuracy: 0.8685\n",
            "Epoch 128/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3299 - accuracy: 0.8565\n",
            "Epoch 129/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3046 - accuracy: 0.8689\n",
            "Epoch 130/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3510 - accuracy: 0.8624\n",
            "Epoch 131/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3295 - accuracy: 0.8593\n",
            "Epoch 132/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3009 - accuracy: 0.8976\n",
            "Epoch 133/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3064 - accuracy: 0.8778\n",
            "Epoch 134/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3585 - accuracy: 0.8562\n",
            "Epoch 135/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3031 - accuracy: 0.8428\n",
            "Epoch 136/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3034 - accuracy: 0.8759\n",
            "Epoch 137/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3060 - accuracy: 0.8602\n",
            "Epoch 138/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3077 - accuracy: 0.8711\n",
            "Epoch 139/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3224 - accuracy: 0.8847\n",
            "Epoch 140/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3360 - accuracy: 0.8745\n",
            "Epoch 141/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3052 - accuracy: 0.8685\n",
            "Epoch 142/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3466 - accuracy: 0.8673\n",
            "Epoch 143/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3171 - accuracy: 0.8639\n",
            "Epoch 144/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3070 - accuracy: 0.8612\n",
            "Epoch 145/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3201 - accuracy: 0.8803\n",
            "Epoch 146/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2910 - accuracy: 0.8748\n",
            "Epoch 147/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3144 - accuracy: 0.8737\n",
            "Epoch 148/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2884 - accuracy: 0.8730\n",
            "Epoch 149/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3251 - accuracy: 0.8624\n",
            "Epoch 150/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3071 - accuracy: 0.8664\n",
            "Epoch 151/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3036 - accuracy: 0.8759\n",
            "Epoch 152/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3106 - accuracy: 0.8686\n",
            "Epoch 153/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2988 - accuracy: 0.8658\n",
            "Epoch 154/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3217 - accuracy: 0.8572\n",
            "Epoch 155/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3231 - accuracy: 0.8680\n",
            "Epoch 156/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3057 - accuracy: 0.8596\n",
            "Epoch 157/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3369 - accuracy: 0.8719\n",
            "Epoch 158/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2973 - accuracy: 0.8667\n",
            "Epoch 159/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3098 - accuracy: 0.8651\n",
            "Epoch 160/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2736 - accuracy: 0.8758\n",
            "Epoch 161/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3121 - accuracy: 0.8693\n",
            "Epoch 162/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2994 - accuracy: 0.8692\n",
            "Epoch 163/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2986 - accuracy: 0.8880\n",
            "Epoch 164/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3025 - accuracy: 0.8645\n",
            "Epoch 165/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3203 - accuracy: 0.8797\n",
            "Epoch 166/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3171 - accuracy: 0.8717\n",
            "Epoch 167/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2907 - accuracy: 0.8680\n",
            "Epoch 168/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3319 - accuracy: 0.8658\n",
            "Epoch 169/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3185 - accuracy: 0.8667\n",
            "Epoch 170/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2923 - accuracy: 0.8714\n",
            "Epoch 171/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2881 - accuracy: 0.8656\n",
            "Epoch 172/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3095 - accuracy: 0.8644\n",
            "Epoch 173/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2826 - accuracy: 0.8669\n",
            "Epoch 174/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3168 - accuracy: 0.8732\n",
            "Epoch 175/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3019 - accuracy: 0.8670\n",
            "Epoch 176/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3252 - accuracy: 0.8596\n",
            "Epoch 177/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3092 - accuracy: 0.8615\n",
            "Epoch 178/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3213 - accuracy: 0.8494\n",
            "Epoch 179/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3032 - accuracy: 0.8547\n",
            "Epoch 180/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2913 - accuracy: 0.8779\n",
            "Epoch 181/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3292 - accuracy: 0.8748\n",
            "Epoch 182/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2904 - accuracy: 0.8745\n",
            "Epoch 183/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3649 - accuracy: 0.8713\n",
            "Epoch 184/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2792 - accuracy: 0.8791\n",
            "Epoch 185/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2954 - accuracy: 0.8644\n",
            "Epoch 186/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3084 - accuracy: 0.8487\n",
            "Epoch 187/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2962 - accuracy: 0.8858\n",
            "Epoch 188/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3261 - accuracy: 0.8628\n",
            "Epoch 189/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3019 - accuracy: 0.8714\n",
            "Epoch 190/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2874 - accuracy: 0.8684\n",
            "Epoch 191/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3020 - accuracy: 0.8560\n",
            "Epoch 192/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2809 - accuracy: 0.8752\n",
            "Epoch 193/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3170 - accuracy: 0.8657\n",
            "Epoch 194/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3002 - accuracy: 0.8844\n",
            "Epoch 195/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2925 - accuracy: 0.8764\n",
            "Epoch 196/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3097 - accuracy: 0.8580\n",
            "Epoch 197/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2815 - accuracy: 0.8803\n",
            "Epoch 198/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2926 - accuracy: 0.8787\n",
            "Epoch 199/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2765 - accuracy: 0.8654\n",
            "Epoch 200/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3571 - accuracy: 0.8803\n",
            "<class 'numpy.ndarray'>\n",
            "(127, 2)\n",
            "----------------------------------------------------------------------\n",
            "classification report for current fold:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.87      0.87        87\n",
            "           1       0.72      0.72      0.73        40\n",
            "\n",
            "    accuracy                           0.83       127\n",
            "   macro avg       0.80      0.80      0.80       127\n",
            "weighted avg       0.83      0.83      0.83       127\n",
            "\n",
            "Area Under ROC (AUC) for the current fold: 0.8864942528735632\n",
            "Confusion Matrix for current fold: \n",
            "[[76 11]\n",
            " [11 29]]\n",
            "Accuracy for Current Fold: 0.8267716535433071\n",
            "Epoch 1/200\n",
            "64/64 [==============================] - 1s 1ms/step - loss: 0.6839 - accuracy: 0.5737\n",
            "Epoch 2/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6480 - accuracy: 0.6709\n",
            "Epoch 3/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6340 - accuracy: 0.6833\n",
            "Epoch 4/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6144 - accuracy: 0.6660\n",
            "Epoch 5/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6116 - accuracy: 0.6660\n",
            "Epoch 6/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6177 - accuracy: 0.6660\n",
            "Epoch 7/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6257 - accuracy: 0.6660\n",
            "Epoch 8/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6052 - accuracy: 0.6660\n",
            "Epoch 9/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6125 - accuracy: 0.6660\n",
            "Epoch 10/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.5857 - accuracy: 0.6660\n",
            "Epoch 11/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6086 - accuracy: 0.6660\n",
            "Epoch 12/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6003 - accuracy: 0.6660\n",
            "Epoch 13/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5976 - accuracy: 0.6660\n",
            "Epoch 14/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5943 - accuracy: 0.6660\n",
            "Epoch 15/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5929 - accuracy: 0.6660\n",
            "Epoch 16/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5689 - accuracy: 0.6660\n",
            "Epoch 17/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5813 - accuracy: 0.6660\n",
            "Epoch 18/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5785 - accuracy: 0.6660\n",
            "Epoch 19/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5872 - accuracy: 0.6660\n",
            "Epoch 20/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5677 - accuracy: 0.6660\n",
            "Epoch 21/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.5719 - accuracy: 0.6660\n",
            "Epoch 22/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5744 - accuracy: 0.6660\n",
            "Epoch 23/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5691 - accuracy: 0.6660\n",
            "Epoch 24/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5782 - accuracy: 0.6660\n",
            "Epoch 25/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5586 - accuracy: 0.6660\n",
            "Epoch 26/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5743 - accuracy: 0.6463\n",
            "Epoch 27/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5823 - accuracy: 0.6660\n",
            "Epoch 28/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.5482 - accuracy: 0.6641\n",
            "Epoch 29/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5576 - accuracy: 0.6821\n",
            "Epoch 30/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5720 - accuracy: 0.6702\n",
            "Epoch 31/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5837 - accuracy: 0.6680\n",
            "Epoch 32/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5669 - accuracy: 0.6660\n",
            "Epoch 33/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6428 - accuracy: 0.6660\n",
            "Epoch 34/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5819 - accuracy: 0.6660\n",
            "Epoch 35/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5713 - accuracy: 0.6660\n",
            "Epoch 36/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5462 - accuracy: 0.6660\n",
            "Epoch 37/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5664 - accuracy: 0.6634\n",
            "Epoch 38/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5564 - accuracy: 0.6556\n",
            "Epoch 39/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5536 - accuracy: 0.6616\n",
            "Epoch 40/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5674 - accuracy: 0.6887\n",
            "Epoch 41/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5736 - accuracy: 0.6973\n",
            "Epoch 42/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5773 - accuracy: 0.6660\n",
            "Epoch 43/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5587 - accuracy: 0.6660\n",
            "Epoch 44/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5717 - accuracy: 0.6660\n",
            "Epoch 45/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5529 - accuracy: 0.6660\n",
            "Epoch 46/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5611 - accuracy: 0.6660\n",
            "Epoch 47/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5864 - accuracy: 0.6660\n",
            "Epoch 48/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6335 - accuracy: 0.6660\n",
            "Epoch 49/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6152 - accuracy: 0.6660\n",
            "Epoch 50/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5649 - accuracy: 0.6660\n",
            "Epoch 51/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5517 - accuracy: 0.6660\n",
            "Epoch 52/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5474 - accuracy: 0.6660\n",
            "Epoch 53/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5619 - accuracy: 0.6660\n",
            "Epoch 54/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5592 - accuracy: 0.6643\n",
            "Epoch 55/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5696 - accuracy: 0.6951\n",
            "Epoch 56/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5606 - accuracy: 0.6796\n",
            "Epoch 57/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5573 - accuracy: 0.6980\n",
            "Epoch 58/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5787 - accuracy: 0.6423\n",
            "Epoch 59/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5396 - accuracy: 0.6838\n",
            "Epoch 60/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5399 - accuracy: 0.6879\n",
            "Epoch 61/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5598 - accuracy: 0.6477\n",
            "Epoch 62/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5402 - accuracy: 0.6498\n",
            "Epoch 63/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5329 - accuracy: 0.6758\n",
            "Epoch 64/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.5503 - accuracy: 0.6873\n",
            "Epoch 65/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5358 - accuracy: 0.7101\n",
            "Epoch 66/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5658 - accuracy: 0.6721\n",
            "Epoch 67/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5529 - accuracy: 0.6794\n",
            "Epoch 68/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5348 - accuracy: 0.6745\n",
            "Epoch 69/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5379 - accuracy: 0.6994\n",
            "Epoch 70/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5390 - accuracy: 0.6933\n",
            "Epoch 71/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5106 - accuracy: 0.7316\n",
            "Epoch 72/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5658 - accuracy: 0.6887\n",
            "Epoch 73/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5274 - accuracy: 0.7270\n",
            "Epoch 74/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5746 - accuracy: 0.6935\n",
            "Epoch 75/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6132 - accuracy: 0.6254\n",
            "Epoch 76/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5791 - accuracy: 0.6514\n",
            "Epoch 77/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5568 - accuracy: 0.6660\n",
            "Epoch 78/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5497 - accuracy: 0.6660\n",
            "Epoch 79/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5096 - accuracy: 0.6638\n",
            "Epoch 80/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5287 - accuracy: 0.6920\n",
            "Epoch 81/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5277 - accuracy: 0.6912\n",
            "Epoch 82/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5265 - accuracy: 0.7171\n",
            "Epoch 83/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5375 - accuracy: 0.6935\n",
            "Epoch 84/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5296 - accuracy: 0.6930\n",
            "Epoch 85/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5520 - accuracy: 0.7051\n",
            "Epoch 86/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5731 - accuracy: 0.6747\n",
            "Epoch 87/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5318 - accuracy: 0.7153\n",
            "Epoch 88/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5328 - accuracy: 0.6965\n",
            "Epoch 89/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.5210 - accuracy: 0.7691\n",
            "Epoch 90/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5411 - accuracy: 0.7647\n",
            "Epoch 91/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5440 - accuracy: 0.7014\n",
            "Epoch 92/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5414 - accuracy: 0.7037\n",
            "Epoch 93/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5515 - accuracy: 0.7148\n",
            "Epoch 94/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5329 - accuracy: 0.7200\n",
            "Epoch 95/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5233 - accuracy: 0.7809\n",
            "Epoch 96/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5423 - accuracy: 0.7747\n",
            "Epoch 97/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5113 - accuracy: 0.7733\n",
            "Epoch 98/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5140 - accuracy: 0.7797\n",
            "Epoch 99/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4994 - accuracy: 0.7893\n",
            "Epoch 100/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5226 - accuracy: 0.7843\n",
            "Epoch 101/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5172 - accuracy: 0.7719\n",
            "Epoch 102/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5116 - accuracy: 0.7829\n",
            "Epoch 103/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5456 - accuracy: 0.7562\n",
            "Epoch 104/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4950 - accuracy: 0.7956\n",
            "Epoch 105/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5037 - accuracy: 0.7993\n",
            "Epoch 106/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5049 - accuracy: 0.7890\n",
            "Epoch 107/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5047 - accuracy: 0.7933\n",
            "Epoch 108/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4879 - accuracy: 0.7892\n",
            "Epoch 109/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4967 - accuracy: 0.7811\n",
            "Epoch 110/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5338 - accuracy: 0.7741\n",
            "Epoch 111/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5011 - accuracy: 0.7904\n",
            "Epoch 112/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5484 - accuracy: 0.7070\n",
            "Epoch 113/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5234 - accuracy: 0.7444\n",
            "Epoch 114/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5189 - accuracy: 0.7609\n",
            "Epoch 115/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4829 - accuracy: 0.8166\n",
            "Epoch 116/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5008 - accuracy: 0.8117\n",
            "Epoch 117/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4841 - accuracy: 0.8260\n",
            "Epoch 118/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4866 - accuracy: 0.8202\n",
            "Epoch 119/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4858 - accuracy: 0.8220\n",
            "Epoch 120/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4454 - accuracy: 0.8310\n",
            "Epoch 121/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4846 - accuracy: 0.8237\n",
            "Epoch 122/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4659 - accuracy: 0.8309\n",
            "Epoch 123/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4446 - accuracy: 0.8343\n",
            "Epoch 124/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4095 - accuracy: 0.8451\n",
            "Epoch 125/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4227 - accuracy: 0.8431\n",
            "Epoch 126/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4371 - accuracy: 0.8373\n",
            "Epoch 127/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4561 - accuracy: 0.8422\n",
            "Epoch 128/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4262 - accuracy: 0.8415\n",
            "Epoch 129/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4316 - accuracy: 0.8391\n",
            "Epoch 130/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4300 - accuracy: 0.8490\n",
            "Epoch 131/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4488 - accuracy: 0.8404\n",
            "Epoch 132/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4203 - accuracy: 0.8456\n",
            "Epoch 133/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4217 - accuracy: 0.8473\n",
            "Epoch 134/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4147 - accuracy: 0.8459\n",
            "Epoch 135/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4187 - accuracy: 0.8466\n",
            "Epoch 136/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4355 - accuracy: 0.8439\n",
            "Epoch 137/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4108 - accuracy: 0.8563\n",
            "Epoch 138/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4455 - accuracy: 0.8460\n",
            "Epoch 139/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4043 - accuracy: 0.8536\n",
            "Epoch 140/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4259 - accuracy: 0.8471\n",
            "Epoch 141/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4330 - accuracy: 0.8428\n",
            "Epoch 142/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4530 - accuracy: 0.8371\n",
            "Epoch 143/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4269 - accuracy: 0.8444\n",
            "Epoch 144/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4099 - accuracy: 0.8500\n",
            "Epoch 145/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4077 - accuracy: 0.8491\n",
            "Epoch 146/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4250 - accuracy: 0.8494\n",
            "Epoch 147/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4339 - accuracy: 0.8455\n",
            "Epoch 148/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4237 - accuracy: 0.8444\n",
            "Epoch 149/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4103 - accuracy: 0.8438\n",
            "Epoch 150/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4156 - accuracy: 0.8455\n",
            "Epoch 151/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4026 - accuracy: 0.8501\n",
            "Epoch 152/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4371 - accuracy: 0.8387\n",
            "Epoch 153/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4006 - accuracy: 0.8430\n",
            "Epoch 154/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4192 - accuracy: 0.8396\n",
            "Epoch 155/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4480 - accuracy: 0.8276\n",
            "Epoch 156/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4339 - accuracy: 0.8262\n",
            "Epoch 157/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4315 - accuracy: 0.8440\n",
            "Epoch 158/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4413 - accuracy: 0.8305\n",
            "Epoch 159/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4241 - accuracy: 0.8497\n",
            "Epoch 160/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4244 - accuracy: 0.8417\n",
            "Epoch 161/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4141 - accuracy: 0.8427\n",
            "Epoch 162/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4048 - accuracy: 0.8312\n",
            "Epoch 163/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4143 - accuracy: 0.8432\n",
            "Epoch 164/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4439 - accuracy: 0.8291\n",
            "Epoch 165/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4004 - accuracy: 0.8430\n",
            "Epoch 166/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4272 - accuracy: 0.8297\n",
            "Epoch 167/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4271 - accuracy: 0.8316\n",
            "Epoch 168/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4067 - accuracy: 0.8277\n",
            "Epoch 169/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4478 - accuracy: 0.8228\n",
            "Epoch 170/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4334 - accuracy: 0.8182\n",
            "Epoch 171/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4208 - accuracy: 0.8449\n",
            "Epoch 172/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4618 - accuracy: 0.8380\n",
            "Epoch 173/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3986 - accuracy: 0.8432\n",
            "Epoch 174/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4206 - accuracy: 0.8492\n",
            "Epoch 175/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4113 - accuracy: 0.8482\n",
            "Epoch 176/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4679 - accuracy: 0.7999\n",
            "Epoch 177/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4203 - accuracy: 0.8248\n",
            "Epoch 178/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4298 - accuracy: 0.8249\n",
            "Epoch 179/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4595 - accuracy: 0.8263\n",
            "Epoch 180/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4407 - accuracy: 0.8390\n",
            "Epoch 181/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4115 - accuracy: 0.8401\n",
            "Epoch 182/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4321 - accuracy: 0.8242\n",
            "Epoch 183/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4178 - accuracy: 0.8376\n",
            "Epoch 184/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4474 - accuracy: 0.8174\n",
            "Epoch 185/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4732 - accuracy: 0.8286\n",
            "Epoch 186/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4329 - accuracy: 0.8254\n",
            "Epoch 187/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4310 - accuracy: 0.8272\n",
            "Epoch 188/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3934 - accuracy: 0.8541\n",
            "Epoch 189/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4107 - accuracy: 0.8251\n",
            "Epoch 190/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4277 - accuracy: 0.8239\n",
            "Epoch 191/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4394 - accuracy: 0.8243\n",
            "Epoch 192/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4263 - accuracy: 0.8273\n",
            "Epoch 193/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4234 - accuracy: 0.8291\n",
            "Epoch 194/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4239 - accuracy: 0.8327\n",
            "Epoch 195/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4154 - accuracy: 0.8303\n",
            "Epoch 196/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4190 - accuracy: 0.8311\n",
            "Epoch 197/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4080 - accuracy: 0.8294\n",
            "Epoch 198/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4233 - accuracy: 0.8121\n",
            "Epoch 199/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4369 - accuracy: 0.8480\n",
            "Epoch 200/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4340 - accuracy: 0.8377\n",
            "<class 'numpy.ndarray'>\n",
            "(127, 2)\n",
            "----------------------------------------------------------------------\n",
            "classification report for current fold:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.84      0.88        87\n",
            "           1       0.71      0.88      0.79        40\n",
            "\n",
            "    accuracy                           0.85       127\n",
            "   macro avg       0.83      0.86      0.84       127\n",
            "weighted avg       0.87      0.85      0.85       127\n",
            "\n",
            "Area Under ROC (AUC) for the current fold: 0.8793103448275862\n",
            "Confusion Matrix for current fold: \n",
            "[[73 14]\n",
            " [ 5 35]]\n",
            "Accuracy for Current Fold: 0.8503937007874016\n",
            "Epoch 1/200\n",
            "64/64 [==============================] - 1s 2ms/step - loss: 0.6858 - accuracy: 0.5979\n",
            "Epoch 2/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6783 - accuracy: 0.6601\n",
            "Epoch 3/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6262 - accuracy: 0.6601\n",
            "Epoch 4/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6213 - accuracy: 0.6601\n",
            "Epoch 5/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6255 - accuracy: 0.6601\n",
            "Epoch 6/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6405 - accuracy: 0.6601\n",
            "Epoch 7/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6221 - accuracy: 0.6601\n",
            "Epoch 8/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6305 - accuracy: 0.6601\n",
            "Epoch 9/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6257 - accuracy: 0.6601\n",
            "Epoch 10/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6029 - accuracy: 0.6601\n",
            "Epoch 11/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6040 - accuracy: 0.6641\n",
            "Epoch 12/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5823 - accuracy: 0.6943\n",
            "Epoch 13/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.5798 - accuracy: 0.7015\n",
            "Epoch 14/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5729 - accuracy: 0.7123\n",
            "Epoch 15/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5548 - accuracy: 0.7554\n",
            "Epoch 16/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5632 - accuracy: 0.7454\n",
            "Epoch 17/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5421 - accuracy: 0.7525\n",
            "Epoch 18/200\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.5397 - accuracy: 0.7474\n",
            "Epoch 19/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5260 - accuracy: 0.7594\n",
            "Epoch 20/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5199 - accuracy: 0.7641\n",
            "Epoch 21/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4995 - accuracy: 0.7911\n",
            "Epoch 22/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5163 - accuracy: 0.7568\n",
            "Epoch 23/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5052 - accuracy: 0.7742\n",
            "Epoch 24/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4818 - accuracy: 0.8143\n",
            "Epoch 25/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4678 - accuracy: 0.7873\n",
            "Epoch 26/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4279 - accuracy: 0.8414\n",
            "Epoch 27/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4459 - accuracy: 0.8345\n",
            "Epoch 28/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4441 - accuracy: 0.8326\n",
            "Epoch 29/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4208 - accuracy: 0.8296\n",
            "Epoch 30/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4337 - accuracy: 0.8488\n",
            "Epoch 31/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4549 - accuracy: 0.8249\n",
            "Epoch 32/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4211 - accuracy: 0.8364\n",
            "Epoch 33/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4322 - accuracy: 0.8311\n",
            "Epoch 34/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4298 - accuracy: 0.8572\n",
            "Epoch 35/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4581 - accuracy: 0.8414\n",
            "Epoch 36/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4345 - accuracy: 0.8325\n",
            "Epoch 37/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3973 - accuracy: 0.8487\n",
            "Epoch 38/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4099 - accuracy: 0.8360\n",
            "Epoch 39/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3810 - accuracy: 0.8500\n",
            "Epoch 40/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3941 - accuracy: 0.8535\n",
            "Epoch 41/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3860 - accuracy: 0.8448\n",
            "Epoch 42/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3947 - accuracy: 0.8340\n",
            "Epoch 43/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4166 - accuracy: 0.8199\n",
            "Epoch 44/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3923 - accuracy: 0.8521\n",
            "Epoch 45/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4227 - accuracy: 0.8445\n",
            "Epoch 46/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3963 - accuracy: 0.8562\n",
            "Epoch 47/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3805 - accuracy: 0.8523\n",
            "Epoch 48/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3950 - accuracy: 0.8424\n",
            "Epoch 49/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4164 - accuracy: 0.8503\n",
            "Epoch 50/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4059 - accuracy: 0.8625\n",
            "Epoch 51/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4092 - accuracy: 0.8511\n",
            "Epoch 52/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4022 - accuracy: 0.8490\n",
            "Epoch 53/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4010 - accuracy: 0.8437\n",
            "Epoch 54/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4164 - accuracy: 0.8553\n",
            "Epoch 55/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4041 - accuracy: 0.8405\n",
            "Epoch 56/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3817 - accuracy: 0.8495\n",
            "Epoch 57/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4228 - accuracy: 0.8483\n",
            "Epoch 58/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4214 - accuracy: 0.8588\n",
            "Epoch 59/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4111 - accuracy: 0.8469\n",
            "Epoch 60/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4188 - accuracy: 0.8546\n",
            "Epoch 61/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3841 - accuracy: 0.8540\n",
            "Epoch 62/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3503 - accuracy: 0.8751\n",
            "Epoch 63/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3889 - accuracy: 0.8527\n",
            "Epoch 64/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3985 - accuracy: 0.8437\n",
            "Epoch 65/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3954 - accuracy: 0.8516\n",
            "Epoch 66/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3717 - accuracy: 0.8465\n",
            "Epoch 67/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3805 - accuracy: 0.8705\n",
            "Epoch 68/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4096 - accuracy: 0.8514\n",
            "Epoch 69/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3606 - accuracy: 0.8615\n",
            "Epoch 70/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3862 - accuracy: 0.8694\n",
            "Epoch 71/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3730 - accuracy: 0.8564\n",
            "Epoch 72/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3741 - accuracy: 0.8602\n",
            "Epoch 73/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3616 - accuracy: 0.8552\n",
            "Epoch 74/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3812 - accuracy: 0.8503\n",
            "Epoch 75/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3729 - accuracy: 0.8674\n",
            "Epoch 76/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3735 - accuracy: 0.8576\n",
            "Epoch 77/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3984 - accuracy: 0.8616\n",
            "Epoch 78/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3841 - accuracy: 0.8599\n",
            "Epoch 79/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3612 - accuracy: 0.8698\n",
            "Epoch 80/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3592 - accuracy: 0.8656\n",
            "Epoch 81/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3859 - accuracy: 0.8595\n",
            "Epoch 82/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3789 - accuracy: 0.8513\n",
            "Epoch 83/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3581 - accuracy: 0.8803\n",
            "Epoch 84/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3856 - accuracy: 0.8696\n",
            "Epoch 85/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3492 - accuracy: 0.8693\n",
            "Epoch 86/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3737 - accuracy: 0.8662\n",
            "Epoch 87/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3591 - accuracy: 0.8768\n",
            "Epoch 88/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3683 - accuracy: 0.8683\n",
            "Epoch 89/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3490 - accuracy: 0.8745\n",
            "Epoch 90/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3489 - accuracy: 0.8675\n",
            "Epoch 91/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3463 - accuracy: 0.8776\n",
            "Epoch 92/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3553 - accuracy: 0.8706\n",
            "Epoch 93/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3825 - accuracy: 0.8736\n",
            "Epoch 94/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3527 - accuracy: 0.8725\n",
            "Epoch 95/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3864 - accuracy: 0.8613\n",
            "Epoch 96/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3484 - accuracy: 0.8653\n",
            "Epoch 97/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3673 - accuracy: 0.8657\n",
            "Epoch 98/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3902 - accuracy: 0.8714\n",
            "Epoch 99/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3560 - accuracy: 0.8715\n",
            "Epoch 100/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3432 - accuracy: 0.8789\n",
            "Epoch 101/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3634 - accuracy: 0.8688\n",
            "Epoch 102/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3805 - accuracy: 0.8665\n",
            "Epoch 103/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3671 - accuracy: 0.8772\n",
            "Epoch 104/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3546 - accuracy: 0.8722\n",
            "Epoch 105/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3713 - accuracy: 0.8655\n",
            "Epoch 106/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3585 - accuracy: 0.8788\n",
            "Epoch 107/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3802 - accuracy: 0.8668\n",
            "Epoch 108/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3563 - accuracy: 0.8675\n",
            "Epoch 109/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3517 - accuracy: 0.8768\n",
            "Epoch 110/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3337 - accuracy: 0.8711\n",
            "Epoch 111/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3546 - accuracy: 0.8692\n",
            "Epoch 112/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3627 - accuracy: 0.8594\n",
            "Epoch 113/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3583 - accuracy: 0.8755\n",
            "Epoch 114/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3523 - accuracy: 0.8650\n",
            "Epoch 115/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3377 - accuracy: 0.8718\n",
            "Epoch 116/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3579 - accuracy: 0.8698\n",
            "Epoch 117/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3650 - accuracy: 0.8620\n",
            "Epoch 118/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3733 - accuracy: 0.8805\n",
            "Epoch 119/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3586 - accuracy: 0.8681\n",
            "Epoch 120/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3648 - accuracy: 0.8685\n",
            "Epoch 121/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3499 - accuracy: 0.8749\n",
            "Epoch 122/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3502 - accuracy: 0.8769\n",
            "Epoch 123/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3673 - accuracy: 0.8736\n",
            "Epoch 124/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3349 - accuracy: 0.8712\n",
            "Epoch 125/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3579 - accuracy: 0.8721\n",
            "Epoch 126/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3429 - accuracy: 0.8653\n",
            "Epoch 127/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3732 - accuracy: 0.8628\n",
            "Epoch 128/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3598 - accuracy: 0.8558\n",
            "Epoch 129/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3452 - accuracy: 0.8755\n",
            "Epoch 130/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3617 - accuracy: 0.8720\n",
            "Epoch 131/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3611 - accuracy: 0.8757\n",
            "Epoch 132/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3717 - accuracy: 0.8594\n",
            "Epoch 133/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3455 - accuracy: 0.8752\n",
            "Epoch 134/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3742 - accuracy: 0.8742\n",
            "Epoch 135/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3468 - accuracy: 0.8749\n",
            "Epoch 136/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3351 - accuracy: 0.8716\n",
            "Epoch 137/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3259 - accuracy: 0.8704\n",
            "Epoch 138/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3626 - accuracy: 0.8662\n",
            "Epoch 139/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3827 - accuracy: 0.8732\n",
            "Epoch 140/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3553 - accuracy: 0.8571\n",
            "Epoch 141/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3338 - accuracy: 0.8842\n",
            "Epoch 142/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3546 - accuracy: 0.8720\n",
            "Epoch 143/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3334 - accuracy: 0.8647\n",
            "Epoch 144/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3522 - accuracy: 0.8731\n",
            "Epoch 145/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3714 - accuracy: 0.8693\n",
            "Epoch 146/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3545 - accuracy: 0.8704\n",
            "Epoch 147/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3402 - accuracy: 0.8774\n",
            "Epoch 148/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3491 - accuracy: 0.8724\n",
            "Epoch 149/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3384 - accuracy: 0.8660\n",
            "Epoch 150/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3344 - accuracy: 0.8629\n",
            "Epoch 151/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4159 - accuracy: 0.8662\n",
            "Epoch 152/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3366 - accuracy: 0.8755\n",
            "Epoch 153/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3442 - accuracy: 0.8758\n",
            "Epoch 154/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3420 - accuracy: 0.8666\n",
            "Epoch 155/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3294 - accuracy: 0.8768\n",
            "Epoch 156/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3557 - accuracy: 0.8764\n",
            "Epoch 157/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3443 - accuracy: 0.8765\n",
            "Epoch 158/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3688 - accuracy: 0.8810\n",
            "Epoch 159/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3336 - accuracy: 0.8676\n",
            "Epoch 160/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3931 - accuracy: 0.8641\n",
            "Epoch 161/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3420 - accuracy: 0.8715\n",
            "Epoch 162/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3577 - accuracy: 0.8625\n",
            "Epoch 163/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3734 - accuracy: 0.8646\n",
            "Epoch 164/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3224 - accuracy: 0.8781\n",
            "Epoch 165/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3558 - accuracy: 0.8585\n",
            "Epoch 166/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3296 - accuracy: 0.8813\n",
            "Epoch 167/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3878 - accuracy: 0.8722\n",
            "Epoch 168/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3892 - accuracy: 0.8520\n",
            "Epoch 169/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3320 - accuracy: 0.8736\n",
            "Epoch 170/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3535 - accuracy: 0.8747\n",
            "Epoch 171/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3316 - accuracy: 0.8799\n",
            "Epoch 172/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3220 - accuracy: 0.8746\n",
            "Epoch 173/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3256 - accuracy: 0.8724\n",
            "Epoch 174/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3371 - accuracy: 0.8767\n",
            "Epoch 175/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3352 - accuracy: 0.8635\n",
            "Epoch 176/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3447 - accuracy: 0.8641\n",
            "Epoch 177/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3252 - accuracy: 0.8735\n",
            "Epoch 178/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3289 - accuracy: 0.8665\n",
            "Epoch 179/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3292 - accuracy: 0.8667\n",
            "Epoch 180/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3130 - accuracy: 0.8682\n",
            "Epoch 181/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3582 - accuracy: 0.8759\n",
            "Epoch 182/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3241 - accuracy: 0.8758\n",
            "Epoch 183/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3709 - accuracy: 0.8632\n",
            "Epoch 184/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3413 - accuracy: 0.8654\n",
            "Epoch 185/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3167 - accuracy: 0.8773\n",
            "Epoch 186/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3118 - accuracy: 0.8688\n",
            "Epoch 187/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3244 - accuracy: 0.8644\n",
            "Epoch 188/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3569 - accuracy: 0.8643\n",
            "Epoch 189/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3370 - accuracy: 0.8590\n",
            "Epoch 190/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3347 - accuracy: 0.8677\n",
            "Epoch 191/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3488 - accuracy: 0.8661\n",
            "Epoch 192/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3149 - accuracy: 0.8814\n",
            "Epoch 193/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3484 - accuracy: 0.8752\n",
            "Epoch 194/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3279 - accuracy: 0.8708\n",
            "Epoch 195/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3125 - accuracy: 0.8738\n",
            "Epoch 196/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3220 - accuracy: 0.8725\n",
            "Epoch 197/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3132 - accuracy: 0.8631\n",
            "Epoch 198/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3165 - accuracy: 0.8770\n",
            "Epoch 199/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3393 - accuracy: 0.8753\n",
            "Epoch 200/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3355 - accuracy: 0.8684\n",
            "<class 'numpy.ndarray'>\n",
            "(127, 2)\n",
            "----------------------------------------------------------------------\n",
            "classification report for current fold:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.88      0.90        88\n",
            "           1       0.75      0.85      0.80        39\n",
            "\n",
            "    accuracy                           0.87       127\n",
            "   macro avg       0.84      0.86      0.85       127\n",
            "weighted avg       0.87      0.87      0.87       127\n",
            "\n",
            "Area Under ROC (AUC) for the current fold: 0.9131701631701632\n",
            "Confusion Matrix for current fold: \n",
            "[[77 11]\n",
            " [ 6 33]]\n",
            "Accuracy for Current Fold: 0.8661417322834646\n",
            "Epoch 1/200\n",
            "64/64 [==============================] - 1s 1ms/step - loss: 0.6833 - accuracy: 0.5905\n",
            "Epoch 2/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6642 - accuracy: 0.6661\n",
            "Epoch 3/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6468 - accuracy: 0.6743\n",
            "Epoch 4/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6382 - accuracy: 0.5958\n",
            "Epoch 5/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6184 - accuracy: 0.6752\n",
            "Epoch 6/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.6030 - accuracy: 0.6870\n",
            "Epoch 7/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5976 - accuracy: 0.7073\n",
            "Epoch 8/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5931 - accuracy: 0.6973\n",
            "Epoch 9/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5499 - accuracy: 0.7361\n",
            "Epoch 10/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5415 - accuracy: 0.7825\n",
            "Epoch 11/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7589\n",
            "Epoch 12/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5083 - accuracy: 0.7712\n",
            "Epoch 13/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5139 - accuracy: 0.7712\n",
            "Epoch 14/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4796 - accuracy: 0.8036\n",
            "Epoch 15/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.5044 - accuracy: 0.8069\n",
            "Epoch 16/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4731 - accuracy: 0.8301\n",
            "Epoch 17/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4564 - accuracy: 0.8296\n",
            "Epoch 18/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4772 - accuracy: 0.8291\n",
            "Epoch 19/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4557 - accuracy: 0.8290\n",
            "Epoch 20/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4587 - accuracy: 0.8496\n",
            "Epoch 21/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4661 - accuracy: 0.8098\n",
            "Epoch 22/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4265 - accuracy: 0.8445\n",
            "Epoch 23/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4489 - accuracy: 0.8517\n",
            "Epoch 24/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4678 - accuracy: 0.8569\n",
            "Epoch 25/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4397 - accuracy: 0.8501\n",
            "Epoch 26/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4352 - accuracy: 0.8505\n",
            "Epoch 27/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4110 - accuracy: 0.8444\n",
            "Epoch 28/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4409 - accuracy: 0.8340\n",
            "Epoch 29/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4028 - accuracy: 0.8598\n",
            "Epoch 30/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4661 - accuracy: 0.8343\n",
            "Epoch 31/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4282 - accuracy: 0.8598\n",
            "Epoch 32/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4242 - accuracy: 0.8640\n",
            "Epoch 33/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4123 - accuracy: 0.8485\n",
            "Epoch 34/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4188 - accuracy: 0.8486\n",
            "Epoch 35/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3861 - accuracy: 0.8566\n",
            "Epoch 36/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3991 - accuracy: 0.8581\n",
            "Epoch 37/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4154 - accuracy: 0.8648\n",
            "Epoch 38/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4257 - accuracy: 0.8509\n",
            "Epoch 39/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4268 - accuracy: 0.8623\n",
            "Epoch 40/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4202 - accuracy: 0.8410\n",
            "Epoch 41/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3925 - accuracy: 0.8612\n",
            "Epoch 42/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3964 - accuracy: 0.8587\n",
            "Epoch 43/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3820 - accuracy: 0.8631\n",
            "Epoch 44/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4292 - accuracy: 0.8499\n",
            "Epoch 45/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3643 - accuracy: 0.8859\n",
            "Epoch 46/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4250 - accuracy: 0.8596\n",
            "Epoch 47/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4164 - accuracy: 0.8678\n",
            "Epoch 48/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3626 - accuracy: 0.8560\n",
            "Epoch 49/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3621 - accuracy: 0.8631\n",
            "Epoch 50/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4032 - accuracy: 0.8571\n",
            "Epoch 51/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3907 - accuracy: 0.8727\n",
            "Epoch 52/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4041 - accuracy: 0.8620\n",
            "Epoch 53/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3964 - accuracy: 0.8633\n",
            "Epoch 54/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4034 - accuracy: 0.8584\n",
            "Epoch 55/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3890 - accuracy: 0.8486\n",
            "Epoch 56/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4011 - accuracy: 0.8675\n",
            "Epoch 57/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3738 - accuracy: 0.8601\n",
            "Epoch 58/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3779 - accuracy: 0.8623\n",
            "Epoch 59/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3445 - accuracy: 0.8726\n",
            "Epoch 60/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4180 - accuracy: 0.8665\n",
            "Epoch 61/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3848 - accuracy: 0.8649\n",
            "Epoch 62/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3900 - accuracy: 0.8654\n",
            "Epoch 63/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3926 - accuracy: 0.8697\n",
            "Epoch 64/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3741 - accuracy: 0.8690\n",
            "Epoch 65/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3913 - accuracy: 0.8623\n",
            "Epoch 66/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3892 - accuracy: 0.8677\n",
            "Epoch 67/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3780 - accuracy: 0.8563\n",
            "Epoch 68/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4159 - accuracy: 0.8701\n",
            "Epoch 69/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3975 - accuracy: 0.8656\n",
            "Epoch 70/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3886 - accuracy: 0.8616\n",
            "Epoch 71/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3837 - accuracy: 0.8626\n",
            "Epoch 72/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4079 - accuracy: 0.8580\n",
            "Epoch 73/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3774 - accuracy: 0.8757\n",
            "Epoch 74/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4069 - accuracy: 0.8708\n",
            "Epoch 75/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3902 - accuracy: 0.8552\n",
            "Epoch 76/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3759 - accuracy: 0.8606\n",
            "Epoch 77/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3958 - accuracy: 0.8706\n",
            "Epoch 78/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3834 - accuracy: 0.8618\n",
            "Epoch 79/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3699 - accuracy: 0.8712\n",
            "Epoch 80/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4136 - accuracy: 0.8566\n",
            "Epoch 81/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3338 - accuracy: 0.8571\n",
            "Epoch 82/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3654 - accuracy: 0.8646\n",
            "Epoch 83/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4006 - accuracy: 0.8631\n",
            "Epoch 84/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3519 - accuracy: 0.8793\n",
            "Epoch 85/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3431 - accuracy: 0.8681\n",
            "Epoch 86/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3556 - accuracy: 0.8608\n",
            "Epoch 87/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3553 - accuracy: 0.8619\n",
            "Epoch 88/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3878 - accuracy: 0.8611\n",
            "Epoch 89/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3873 - accuracy: 0.8673\n",
            "Epoch 90/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3791 - accuracy: 0.8651\n",
            "Epoch 91/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3615 - accuracy: 0.8691\n",
            "Epoch 92/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3354 - accuracy: 0.8637\n",
            "Epoch 93/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3749 - accuracy: 0.8619\n",
            "Epoch 94/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3486 - accuracy: 0.8689\n",
            "Epoch 95/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4187 - accuracy: 0.8602\n",
            "Epoch 96/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3553 - accuracy: 0.8608\n",
            "Epoch 97/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3843 - accuracy: 0.8642\n",
            "Epoch 98/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3720 - accuracy: 0.8607\n",
            "Epoch 99/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3522 - accuracy: 0.8650\n",
            "Epoch 100/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3446 - accuracy: 0.8761\n",
            "Epoch 101/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3508 - accuracy: 0.8731\n",
            "Epoch 102/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3494 - accuracy: 0.8616\n",
            "Epoch 103/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3662 - accuracy: 0.8614\n",
            "Epoch 104/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3773 - accuracy: 0.8536\n",
            "Epoch 105/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3880 - accuracy: 0.8560\n",
            "Epoch 106/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3562 - accuracy: 0.8690\n",
            "Epoch 107/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3680 - accuracy: 0.8711\n",
            "Epoch 108/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3725 - accuracy: 0.8544\n",
            "Epoch 109/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3643 - accuracy: 0.8742\n",
            "Epoch 110/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3612 - accuracy: 0.8653\n",
            "Epoch 111/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3569 - accuracy: 0.8626\n",
            "Epoch 112/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3603 - accuracy: 0.8556\n",
            "Epoch 113/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3231 - accuracy: 0.8721\n",
            "Epoch 114/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3602 - accuracy: 0.8706\n",
            "Epoch 115/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3621 - accuracy: 0.8805\n",
            "Epoch 116/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3654 - accuracy: 0.8684\n",
            "Epoch 117/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3689 - accuracy: 0.8754\n",
            "Epoch 118/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3819 - accuracy: 0.8574\n",
            "Epoch 119/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3797 - accuracy: 0.8608\n",
            "Epoch 120/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3550 - accuracy: 0.8681\n",
            "Epoch 121/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3898 - accuracy: 0.8733\n",
            "Epoch 122/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3321 - accuracy: 0.8701\n",
            "Epoch 123/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3595 - accuracy: 0.8768\n",
            "Epoch 124/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3629 - accuracy: 0.8634\n",
            "Epoch 125/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3573 - accuracy: 0.8614\n",
            "Epoch 126/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3440 - accuracy: 0.8768\n",
            "Epoch 127/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3535 - accuracy: 0.8745\n",
            "Epoch 128/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3846 - accuracy: 0.8632\n",
            "Epoch 129/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3881 - accuracy: 0.8511\n",
            "Epoch 130/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3927 - accuracy: 0.8695\n",
            "Epoch 131/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3503 - accuracy: 0.8655\n",
            "Epoch 132/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3319 - accuracy: 0.8681\n",
            "Epoch 133/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3209 - accuracy: 0.8771\n",
            "Epoch 134/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3551 - accuracy: 0.8770\n",
            "Epoch 135/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3307 - accuracy: 0.8838\n",
            "Epoch 136/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3326 - accuracy: 0.8696\n",
            "Epoch 137/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3280 - accuracy: 0.8736\n",
            "Epoch 138/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3350 - accuracy: 0.8649\n",
            "Epoch 139/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3599 - accuracy: 0.8630\n",
            "Epoch 140/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3303 - accuracy: 0.8735\n",
            "Epoch 141/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3264 - accuracy: 0.8696\n",
            "Epoch 142/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3682 - accuracy: 0.8835\n",
            "Epoch 143/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3703 - accuracy: 0.8618\n",
            "Epoch 144/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3190 - accuracy: 0.8716\n",
            "Epoch 145/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3199 - accuracy: 0.8894\n",
            "Epoch 146/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3500 - accuracy: 0.8656\n",
            "Epoch 147/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3409 - accuracy: 0.8729\n",
            "Epoch 148/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3316 - accuracy: 0.8689\n",
            "Epoch 149/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3208 - accuracy: 0.8690\n",
            "Epoch 150/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3503 - accuracy: 0.8786\n",
            "Epoch 151/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3450 - accuracy: 0.8774\n",
            "Epoch 152/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3129 - accuracy: 0.8577\n",
            "Epoch 153/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3441 - accuracy: 0.8606\n",
            "Epoch 154/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3539 - accuracy: 0.8633\n",
            "Epoch 155/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3272 - accuracy: 0.8726\n",
            "Epoch 156/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3237 - accuracy: 0.8665\n",
            "Epoch 157/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3514 - accuracy: 0.8609\n",
            "Epoch 158/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3517 - accuracy: 0.8736\n",
            "Epoch 159/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3467 - accuracy: 0.8472\n",
            "Epoch 160/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3759 - accuracy: 0.8582\n",
            "Epoch 161/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3140 - accuracy: 0.8702\n",
            "Epoch 162/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3717 - accuracy: 0.8671\n",
            "Epoch 163/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3373 - accuracy: 0.8583\n",
            "Epoch 164/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3429 - accuracy: 0.8642\n",
            "Epoch 165/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3586 - accuracy: 0.8713\n",
            "Epoch 166/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3268 - accuracy: 0.8698\n",
            "Epoch 167/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3304 - accuracy: 0.8819\n",
            "Epoch 168/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3350 - accuracy: 0.8541\n",
            "Epoch 169/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3394 - accuracy: 0.8675\n",
            "Epoch 170/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3533 - accuracy: 0.8705\n",
            "Epoch 171/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3443 - accuracy: 0.8513\n",
            "Epoch 172/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3645 - accuracy: 0.8693\n",
            "Epoch 173/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3332 - accuracy: 0.8724\n",
            "Epoch 174/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3518 - accuracy: 0.8556\n",
            "Epoch 175/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3319 - accuracy: 0.8691\n",
            "Epoch 176/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3222 - accuracy: 0.8597\n",
            "Epoch 177/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3345 - accuracy: 0.8719\n",
            "Epoch 178/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3410 - accuracy: 0.8700\n",
            "Epoch 179/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3536 - accuracy: 0.8699\n",
            "Epoch 180/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3216 - accuracy: 0.8730\n",
            "Epoch 181/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3301 - accuracy: 0.8546\n",
            "Epoch 182/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3109 - accuracy: 0.8557\n",
            "Epoch 183/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3171 - accuracy: 0.8593\n",
            "Epoch 184/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3137 - accuracy: 0.8737\n",
            "Epoch 185/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3113 - accuracy: 0.8639\n",
            "Epoch 186/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3463 - accuracy: 0.8625\n",
            "Epoch 187/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3434 - accuracy: 0.8719\n",
            "Epoch 188/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3089 - accuracy: 0.8800\n",
            "Epoch 189/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3290 - accuracy: 0.8673\n",
            "Epoch 190/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3008 - accuracy: 0.8692\n",
            "Epoch 191/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3088 - accuracy: 0.8750\n",
            "Epoch 192/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3463 - accuracy: 0.8645\n",
            "Epoch 193/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3236 - accuracy: 0.8756\n",
            "Epoch 194/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3286 - accuracy: 0.8727\n",
            "Epoch 195/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3316 - accuracy: 0.8658\n",
            "Epoch 196/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3428 - accuracy: 0.8662\n",
            "Epoch 197/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3477 - accuracy: 0.8894\n",
            "Epoch 198/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3475 - accuracy: 0.8583\n",
            "Epoch 199/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.3253 - accuracy: 0.8676\n",
            "Epoch 200/200\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.2990 - accuracy: 0.8812\n",
            "<class 'numpy.ndarray'>\n",
            "(127, 2)\n",
            "----------------------------------------------------------------------\n",
            "classification report for current fold:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.84      0.88        88\n",
            "           1       0.70      0.85      0.77        39\n",
            "\n",
            "    accuracy                           0.84       127\n",
            "   macro avg       0.81      0.84      0.82       127\n",
            "weighted avg       0.86      0.84      0.85       127\n",
            "\n",
            "Area Under ROC (AUC) for the current fold: 0.9018065268065268\n",
            "Confusion Matrix for current fold: \n",
            "[[74 14]\n",
            " [ 6 33]]\n",
            "Accuracy for Current Fold: 0.84251968503937\n",
            "\n",
            "-------------------------------Average-------------------------------\n",
            "AUC (Avg.) is  0.893\n",
            "Accuracy (Avg.) is  0.920\n",
            "Avg. CM is [[31, 11], [7]]\n",
            "Total for all folds CM is [[159, 58], [39, 380]]\n",
            "Precision (Avg. ) is  0.735 \n",
            "Sensitivity (Avg. ) is  0.803 \n",
            "Specificity (Avg. ) is  0.868 \n",
            "FOR (Avg.) is  0.092 \n",
            "DOR (Avg. ) is  29.730 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8d-ZBGtTGdYX"
      },
      "source": [
        ""
      ],
      "execution_count": 19,
      "outputs": []
    }
  ]
}